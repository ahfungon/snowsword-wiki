#!/usr/bin/env python3
"""
æ„å»ºå¤šæºçŸ¥è¯†åº“ - æ•´åˆå°è¯´åŸæ–‡å’Œè§£è¯´å…¨é›†
"""

import sys
from pathlib import Path

sys.path.append('.')

from src.multi_source_indexer import MultiSourceIndexer
from src.knowledge_builder import KnowledgeBuilder
import json


def build_multi_source_knowledge():
    """æ„å»ºå¤šæºçŸ¥è¯†åº“"""
    data_dir = Path("data")
    
    print("ğŸš€ å¼€å§‹æ„å»ºå¤šæºçŸ¥è¯†åº“...")
    print(f"ğŸ“ æ•°æ®ç›®å½•: {data_dir.absolute()}")
    
    # ========== 1. æ£€æŸ¥æºæ–‡ä»¶ ==========
    sources = []
    
    # å°è¯´åŸæ–‡
    novel_path = data_dir / "é›ªä¸­æ‚åˆ€è¡Œ.txt"
    if novel_path.exists():
        sources.append((novel_path, "å°è¯´åŸæ–‡", "novel"))
        print(f"âœ… æ‰¾åˆ°å°è¯´åŸæ–‡: {novel_path.stat().st_size / 1024 / 1024:.2f} MB")
    else:
        print(f"âš ï¸  æ‰¾ä¸åˆ°å°è¯´åŸæ–‡: {novel_path}")
    
    # è§£è¯´å…¨é›†
    commentary_path = data_dir / "é›ªä¸­æ‚åˆ€è¡Œ_è§£è¯´å…¨é›†.txt"
    if commentary_path.exists():
        sources.append((commentary_path, "è§£è¯´å…¨é›†", "commentary"))
        print(f"âœ… æ‰¾åˆ°è§£è¯´å…¨é›†: {commentary_path.stat().st_size / 1024:.2f} KB")
    else:
        print(f"âš ï¸  æ‰¾ä¸åˆ°è§£è¯´å…¨é›†: {commentary_path}")
    
    if not sources:
        print("âŒ é”™è¯¯ï¼šæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æºæ–‡ä»¶")
        return 1
    
    # ========== 2. æ„å»ºå¤šæºç´¢å¼• ==========
    print(f"\n{'='*60}")
    print("ğŸ“š æ­¥éª¤ 1: æ„å»ºå¤šæºæ–‡æœ¬ç´¢å¼•")
    print(f"{'='*60}")
    
    indexer = MultiSourceIndexer(chunk_size=800, overlap=100)
    
    for path, name, source_id in sources:
        try:
            indexer.add_source(path, name, source_id)
        except Exception as e:
            print(f"âŒ å¤„ç† {name} æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
    
    # ä¿å­˜ç´¢å¼•
    indexer.build_all_indexes(str(data_dir))
    
    # ========== 3. æ„å»ºè§£è¯´å…¨é›†çš„çŸ¥è¯†åº“ ==========
    if commentary_path.exists():
        print(f"\n{'='*60}")
        print("ğŸ“– æ­¥éª¤ 2: æ„å»ºè§£è¯´å…¨é›†çŸ¥è¯†åº“")
        print(f"{'='*60}")
        
        try:
            with open(commentary_path, 'r', encoding='utf-8') as f:
                commentary_text = f.read()
            
            # ä½¿ç”¨ KnowledgeBuilder å¤„ç†è§£è¯´å…¨é›†
            kb = KnowledgeBuilder()
            
            # è§£æç« èŠ‚
            print("\nğŸ” è§£æè§£è¯´å…¨é›†ç« èŠ‚...")
            # è§£è¯´å…¨é›†çš„ç« èŠ‚æå–é€»è¾‘
            import re
            
            sections = []
            section_pattern = r'ç¬¬\s*[0-9]+\s*ç« ï¼š[^\n]+'
            parts = re.split(f'({section_pattern})', commentary_text)
            
            current_title = "å‰è¨€"
            current_content = []
            
            for part in parts:
                if not part.strip():
                    continue
                
                if re.match(section_pattern, part.strip()):
                    if current_content:
                        sections.append({
                            'title': current_title,
                            'content': '\n'.join(current_content),
                            'word_count': len(''.join(current_content))
                        })
                    current_title = part.strip()
                    current_content = []
                else:
                    current_content.append(part)
            
            if current_content:
                sections.append({
                    'title': current_title,
                    'content': '\n'.join(current_content),
                    'word_count': len(''.join(current_content))
                })
            
            print(f"   æ‰¾åˆ° {len(sections)} ä¸ªè§£è¯´ç« èŠ‚")
            
            # ç”Ÿæˆç« èŠ‚æ‘˜è¦
            kb.chapters = sections
            summaries = kb.generate_all_summaries()
            kb.save_summaries(data_dir / "commentary_summaries.json")
            
            # æ„å»ºä¸“å®¶çŸ¥è¯†åº“
            kb.build_expert_knowledge()
            kb.save_knowledge_base(data_dir / "commentary_knowledge.json")
            
            print(f"\nâœ… è§£è¯´å…¨é›†çŸ¥è¯†åº“æ„å»ºå®Œæˆ")
            
        except Exception as e:
            print(f"âŒ å¤„ç†è§£è¯´å…¨é›†æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
    
    # ========== 4. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š ==========
    print(f"\n{'='*60}")
    print("ğŸ“Š çŸ¥è¯†åº“ç»Ÿè®¡æŠ¥å‘Š")
    print(f"{'='*60}")
    
    # åŠ è½½ç´¢å¼•ç»Ÿè®¡
    chunks_path = data_dir / "chunks.json"
    if chunks_path.exists():
        with open(chunks_path, 'r', encoding='utf-8') as f:
            chunks = json.load(f)
        
        print(f"\næ€»æ–‡æœ¬å—: {len(chunks)}")
        
        # æŒ‰æ¥æºç»Ÿè®¡
        from collections import Counter
        source_counts = Counter(c['source_name'] for c in chunks)
        for source, count in source_counts.items():
            print(f"  - {source}: {count} ä¸ªæ–‡æœ¬å— ({count/len(chunks)*100:.1f}%)")
    
    # çŸ¥è¯†åº“æ–‡ä»¶
    print(f"\nç”Ÿæˆçš„æ–‡ä»¶:")
    for file in ["chunks.json", "sources.json", "keyword_index.json", 
                 "commentary_summaries.json", "commentary_knowledge.json"]:
        file_path = data_dir / file
        if file_path.exists():
            size = file_path.stat().st_size / 1024
            print(f"  âœ… {file} ({size:.1f} KB)")
    
    print(f"\n{'='*60}")
    print("âœ… å¤šæºçŸ¥è¯†åº“æ„å»ºå®Œæˆï¼")
    print(f"{'='*60}")
    print("\nç°åœ¨å¯ä»¥è¿è¡Œ: streamlit run app.py")
    
    return 0


if __name__ == "__main__":
    sys.exit(build_multi_source_knowledge())