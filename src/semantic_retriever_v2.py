#!/usr/bin/env python3
"""
è¯­ä¹‰æ£€ç´¢å™¨ V2 - è½»é‡ç‰ˆ
ä½¿ç”¨ FlagEmbedding/BGE è¿›è¡Œè¯­ä¹‰æ£€ç´¢
"""

import json
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SemanticRetrieverV2:
    """
    è¯­ä¹‰æ£€ç´¢å™¨ V2
    - ä½¿ç”¨ BGE ä¸­æ–‡ Embedding
    - æ”¯æŒæœ¬åœ°å‘é‡å­˜å‚¨
    """
    
    def __init__(self, model_name: str = "BAAI/bge-base-zh", use_fp16: bool = False):
        self.model_name = model_name
        self.model = None
        self.embeddings = None
        self.metadata = None
        self._load_model(use_fp16)
    
    def _load_model(self, use_fp16: bool = False):
        """åŠ è½½ Embedding æ¨¡å‹"""
        try:
            # å°è¯•ä½¿ç”¨ FlagEmbeddingï¼ˆæ¨èï¼‰
            from FlagEmbedding import FlagModel
            logger.info(f"ğŸ“¥ åŠ è½½ FlagEmbedding æ¨¡å‹: {self.model_name}")
            self.model = FlagModel(self.model_name, use_fp16=use_fp16)
            self.model_type = 'flag'
            logger.info("âœ… FlagEmbedding æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e1:
            logger.warning(f"âš ï¸ FlagEmbedding åŠ è½½å¤±è´¥: {e1}")
            try:
                # å¤‡ç”¨ï¼šä½¿ç”¨ sentence-transformers
                from sentence_transformers import SentenceTransformer
                logger.info(f"ğŸ“¥ åŠ è½½ SentenceTransformer: {self.model_name}")
                self.model = SentenceTransformer(self.model_name)
                self.model_type = 'st'
                logger.info("âœ… SentenceTransformer åŠ è½½æˆåŠŸ")
            except Exception as e2:
                logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e2}")
                raise
    
    def encode(self, texts: List[str]) -> np.ndarray:
        """ç¼–ç æ–‡æœ¬"""
        if isinstance(texts, str):
            texts = [texts]
        
        if self.model_type == 'flag':
            # FlagEmbedding ç¼–ç 
            embeddings = self.model.encode(texts)
        else:
            # SentenceTransformer ç¼–ç 
            embeddings = self.model.encode(texts, show_progress_bar=False)
        
        return embeddings
    
    def build_index(self, paragraphs: List[Dict], output_dir: Path):
        """
        æ„å»ºå‘é‡ç´¢å¼•
        paragraphs: [{'content': ..., 'chapter': ..., ...}, ...]
        """
        logger.info(f"ğŸ”¨ æ„å»ºå‘é‡ç´¢å¼•ï¼Œå…± {len(paragraphs)} ä¸ªæ®µè½")
        
        # æå–æ–‡æœ¬
        texts = [p['content'] for p in paragraphs]
        
        # åˆ†æ‰¹ç¼–ç ï¼ˆé¿å…å†…å­˜é—®é¢˜ï¼‰
        batch_size = 500
        all_embeddings = []
        
        logger.info("ğŸ”„ ç¼–ç æ–‡æœ¬...")
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            embeddings = self.encode(batch)
            all_embeddings.append(embeddings)
            if (i + batch_size) % 2000 == 0:
                logger.info(f"   å·²ç¼–ç : {min(i+batch_size, len(texts))}/{len(texts)}")
        
        # åˆå¹¶
        all_embeddings = np.vstack(all_embeddings)
        
        # ä¿å­˜
        output_dir.mkdir(exist_ok=True, parents=True)
        np.save(output_dir / "embeddings.npy", all_embeddings)
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = [
            {
                'idx': i,
                'chapter': p.get('chapter', ''),
                'chapter_idx': p.get('chapter_idx', 0),
                'paragraph_idx': p.get('paragraph_idx', 0),
                'content': p['content'],
                'length': p.get('length', 0)
            }
            for i, p in enumerate(paragraphs)
        ]
        
        with open(output_dir / "metadata.json", 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        self.embeddings = all_embeddings
        self.metadata = metadata
        
        logger.info(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆ: {output_dir}")
        logger.info(f"   å‘é‡ç»´åº¦: {all_embeddings.shape}")
    
    def load_index(self, index_dir: Path):
        """åŠ è½½ç´¢å¼•"""
        logger.info(f"ğŸ“‚ åŠ è½½ç´¢å¼•: {index_dir}")
        
        self.embeddings = np.load(index_dir / "embeddings.npy")
        with open(index_dir / "metadata.json", 'r', encoding='utf-8') as f:
            self.metadata = json.load(f)
        
        logger.info(f"âœ… ç´¢å¼•åŠ è½½å®Œæˆ: {self.embeddings.shape}")
    
    def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """
        è¯­ä¹‰æ£€ç´¢
        """
        if self.embeddings is None:
            logger.error("âŒ ç´¢å¼•æœªåŠ è½½")
            return []
        
        # ç¼–ç æŸ¥è¯¢
        query_emb = self.encode([query])
        
        # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        similarities = np.dot(self.embeddings, query_emb.T).squeeze()
        
        # å– top-k
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        # ç»„è£…ç»“æœ
        results = []
        for idx in top_indices:
            results.append({
                'idx': int(idx),
                'similarity': float(similarities[idx]),
                **self.metadata[idx]
            })
        
        return results
    
    def hybrid_search(self, query: str, top_k: int = 5, keyword_weight: float = 0.3) -> List[Dict]:
        """
        æ··åˆæ£€ç´¢ï¼šè¯­ä¹‰ + å…³é”®è¯
        """
        # è¯­ä¹‰åˆ†æ•°
        query_emb = self.encode([query])
        semantic_scores = np.dot(self.embeddings, query_emb.T).squeeze()
        
        # å…³é”®è¯åˆ†æ•°
        keywords = set(query.split())
        keyword_scores = np.zeros(len(self.metadata))
        
        for i, meta in enumerate(self.metadata):
            content = meta['content']
            score = sum(1 for kw in keywords if kw in content)
            keyword_scores[i] = score / max(len(keywords), 1)
        
        # èåˆ
        combined_scores = (1 - keyword_weight) * semantic_scores + keyword_weight * keyword_scores
        
        # å– top-k
        top_indices = np.argsort(combined_scores)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            results.append({
                'idx': int(idx),
                'similarity': float(combined_scores[idx]),
                'semantic_score': float(semantic_scores[idx]),
                'keyword_score': float(keyword_scores[idx]),
                **self.metadata[idx]
            })
        
        return results


if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯•è¯­ä¹‰æ£€ç´¢å™¨ V2")
    
    # æµ‹è¯•æ•°æ®
    test_paragraphs = [
        {'content': 'å¾å‡¤å¹´ä¸ºæ¯æŠ¥ä»‡ï¼Œåœ¨å¤ªå®‰åŸå¤–æ–©æ€éŸ©è²‚å¯º', 'chapter': 'ç¬¬100ç« '},
        {'content': 'å§œæ³¥å’Œå¾å‡¤å¹´ä»å°ä¸€èµ·é•¿å¤§ï¼Œæœ€ç»ˆæˆä¸ºåŒ—å‡‰ç‹å¦ƒ', 'chapter': 'ç¬¬200ç« '},
        {'content': 'ææ·³ç½¡ä¼ æˆå¾å‡¤å¹´ä¸¤è¢–é’è›‡ï¼Œå‰‘é“å¤§æˆ', 'chapter': 'ç¬¬50ç« '},
        {'content': 'ç‹ä»™èŠè‡ªç§°å¤©ä¸‹ç¬¬äºŒï¼Œé•‡å®ˆæ­¦å¸åŸä¸€ç”²å­', 'chapter': 'ç¬¬150ç« '},
        {'content': 'å¾éªäººå± ä¹‹åå¨éœ‡å¤©ä¸‹ï¼Œå®ˆæŠ¤åŒ—å‡‰ä¸‰åå¹´', 'chapter': 'ç¬¬10ç« '},
    ]
    
    # åˆ›å»ºæ£€ç´¢å™¨
    retriever = SemanticRetrieverV2()
    
    # æ„å»ºç´¢å¼•
    retriever.build_index(test_paragraphs, Path("test_index"))
    
    # æµ‹è¯•æŸ¥è¯¢
    queries = [
        "å¾å‡¤å¹´ä¸ºä»€ä¹ˆè¦æ€éŸ©è²‚å¯ºï¼Ÿ",
        "å§œæ³¥å’Œå¾å‡¤å¹´çš„å…³ç³»ï¼Ÿ",
        "è°æ•™å¾å‡¤å¹´å‰‘æ³•ï¼Ÿ",
    ]
    
    for query in queries:
        print(f"\nğŸ” æŸ¥è¯¢: {query}")
        results = retriever.search(query, top_k=2)
        for r in results:
            print(f"   [{r['idx']}] {r['content'][:40]}... (ç›¸ä¼¼åº¦: {r['similarity']:.3f})")
