"""
æ£€ç´¢æ¨¡å—ï¼šåŸºäºå…³é”®è¯å’Œè¯­ä¹‰æ£€ç´¢ç›¸å…³æ–‡æœ¬å—
"""

import json
import re
from pathlib import Path
from typing import List, Dict
from collections import Counter
import jieba

class TextRetriever:
    def __init__(self, data_dir: str = "data"):
        self.data_dir = Path(data_dir)
        self.chunks: List[Dict] = []
        self.keyword_index: Dict = {}
        self._load_data()
    
    def _load_data(self):
        """åŠ è½½ç´¢å¼•æ•°æ®ï¼Œæ”¯æŒå‹ç¼©åˆ†å—æ–‡ä»¶è§£å‹åˆå¹¶"""
        chunks_path = self.data_dir / "chunks.json"
        index_path = self.data_dir / "keyword_index.json"
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å‹ç¼©çš„åˆ†å—æ–‡ä»¶
        compressed_files = sorted(self.data_dir.glob("chunks_small_*.gz"))
        
        if not chunks_path.exists() and compressed_files:
            print(f"ğŸ“¦ å‘ç° {len(compressed_files)} ä¸ªå‹ç¼©æ–‡ä»¶ï¼Œå¼€å§‹è§£å‹åˆå¹¶...")
            self._merge_compressed_files(compressed_files, chunks_path)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœªå‹ç¼©çš„åˆ†å—æ–‡ä»¶ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰
        split_files = sorted(self.data_dir.glob("chunks_part_*"))
        if not chunks_path.exists() and split_files:
            print(f"ğŸ“¦ å‘ç° {len(split_files)} ä¸ªåˆ†å—æ–‡ä»¶ï¼Œæ­£åœ¨åˆå¹¶...")
            self._merge_split_files(split_files, chunks_path)
        
        if not chunks_path.exists():
            print("ğŸ“¦ ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ­£åœ¨ä»å°è¯´æ–‡æœ¬æ„å»º...")
            print("â±ï¸ è¿™å¯èƒ½éœ€è¦ 1-2 åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")
            self._build_index()
        else:
            print(f"ğŸ“– æ­£åœ¨åŠ è½½ç´¢å¼•...")
            with open(chunks_path, 'r', encoding='utf-8') as f:
                self.chunks = json.load(f)
            
            if index_path.exists():
                with open(index_path, 'r', encoding='utf-8') as f:
                    self.keyword_index = json.load(index_path)
            
            # åˆ›å»º id åˆ° chunk çš„æ˜ å°„
            self.chunk_map = {chunk['id']: chunk for chunk in self.chunks}
            
            print(f"âœ… åŠ è½½äº† {len(self.chunks)} ä¸ªæ–‡æœ¬å—")
    
    def _merge_compressed_files(self, compressed_files: List[Path], output_path: Path):
        """è§£å‹å¹¶åˆå¹¶å‹ç¼©çš„åˆ†å—æ–‡ä»¶"""
        import gzip
        import subprocess
        
        total_files = len(compressed_files)
        print(f"ğŸ—œï¸  å¼€å§‹è§£å‹ {total_files} ä¸ªæ–‡ä»¶...")
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•å­˜æ”¾è§£å‹åçš„æ–‡ä»¶
        temp_dir = self.data_dir / "temp_chunks"
        temp_dir.mkdir(exist_ok=True)
        
        # è§£å‹æ¯ä¸ªæ–‡ä»¶
        for i, gz_file in enumerate(compressed_files, 1):
            print(f"  [{i}/{total_files}] è§£å‹ {gz_file.name}...", end=" ")
            output_file = temp_dir / gz_file.stem  # å»æ‰ .gz åç¼€
            
            with gzip.open(gz_file, 'rb') as f_in:
                with open(output_file, 'wb') as f_out:
                    f_out.write(f_in.read())
            
            print("âœ…")
        
        # åˆå¹¶æ‰€æœ‰è§£å‹åçš„æ–‡ä»¶
        print(f"ğŸ“‘ åˆå¹¶ {total_files} ä¸ªæ–‡ä»¶...")
        split_files = sorted(temp_dir.glob("chunks_small_*"))
        files_str = ' '.join([str(f) for f in split_files])
        cmd = f"cat {files_str} > {output_path}"
        subprocess.run(cmd, shell=True, check=True)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        for f in split_files:
            f.unlink()
        temp_dir.rmdir()
        
        print(f"âœ… è§£å‹åˆå¹¶å®Œæˆ: {output_path}")
    
    def _merge_split_files(self, split_files: List[Path], output_path: Path):
        """åˆå¹¶åˆ†å—æ–‡ä»¶"""
        import subprocess
        
        print(f"ğŸ“‘ åˆå¹¶ {len(split_files)} ä¸ªæ–‡ä»¶...")
        files_str = ' '.join([str(f) for f in split_files])
        cmd = f"cat {files_str} > {output_path}"
        subprocess.run(cmd, shell=True, check=True)
        
        print(f"âœ… åˆå¹¶å®Œæˆ: {output_path}")
    
    def _build_index(self):
        """è‡ªåŠ¨æ„å»ºç´¢å¼•"""
        from .indexer import TextIndexer
        
        text_file = self.data_dir / "é›ªä¸­æ‚åˆ€è¡Œ.txt"
        if not text_file.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°å°è¯´æ–‡æœ¬æ–‡ä»¶: {text_file}")
        
        print(f"å¼€å§‹æ„å»ºç´¢å¼•ï¼Œæ–‡æœ¬æ–‡ä»¶: {text_file}")
        indexer = TextIndexer(chunk_size=800, overlap=100)
        
        with open(text_file, 'r', encoding='utf-8') as f:
            text = f.read()
        
        self.chunks = indexer.create_chunks(text)
        
        # ä¿å­˜ç´¢å¼•
        chunks_path = self.data_dir / "chunks.json"
        with open(chunks_path, 'w', encoding='utf-8') as f:
            json.dump(self.chunks, f, ensure_ascii=False)
        
        # æ„å»ºå…³é”®è¯ç´¢å¼•
        self.keyword_index = self._build_keyword_index()
        index_path = self.data_dir / "keyword_index.json"
        with open(index_path, 'w', encoding='utf-8') as f:
            json.dump(self.keyword_index, f, ensure_ascii=False)
        
        # åˆ›å»ºæ˜ å°„
        self.chunk_map = {chunk['id']: chunk for chunk in self.chunks}
        
        print(f"ç´¢å¼•æ„å»ºå®Œæˆï¼å…± {len(self.chunks)} ä¸ªæ–‡æœ¬å—")
    
    def _build_keyword_index(self) -> Dict:
        """æ„å»ºå…³é”®è¯ç´¢å¼•"""
        from collections import defaultdict
        
        index = defaultdict(list)
        stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'å—', 'å‘¢', 'å§'}
        
        for chunk in self.chunks:
            words = jieba.lcut(chunk['content'])
            for word in words:
                word = word.strip()
                if len(word) >= 2 and word not in stop_words:
                    index[word].append(chunk['id'])
        
        # å»é‡
        for word in index:
            index[word] = list(set(index[word]))
        
        return dict(index)
    
    def extract_keywords(self, query: str) -> List[str]:
        """
        ä»æŸ¥è¯¢ä¸­æå–å…³é”®è¯
        """
        # ä½¿ç”¨ jieba åˆ†è¯
        words = jieba.lcut(query)
        
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'å—', 'å‘¢', 'å§', 'å—'}
        
        keywords = []
        for word in words:
            word = word.strip()
            if len(word) >= 2 and word not in stop_words:
                keywords.append(word)
        
        # å¦‚æœæ²¡æœ‰æå–åˆ°æœ‰æ•ˆå…³é”®è¯ï¼Œä¿ç•™åŸè¯
        if not keywords:
            keywords = [w for w in words if len(w.strip()) >= 2]
        
        return keywords
    
    def retrieve(self, query: str, top_k: int = 5) -> List[Dict]:
        """
        æ£€ç´¢ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„æ–‡æœ¬å—
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            top_k: è¿”å›æœ€ç›¸å…³çš„ k ä¸ªç»“æœ
            
        Returns:
            List of relevant chunks with scores
        """
        if not self.chunks:
            return []
        
        # æå–æŸ¥è¯¢å…³é”®è¯
        keywords = self.extract_keywords(query)
        
        if not keywords:
            return []
        
        # è®¡ç®—æ¯ä¸ªå—çš„ç›¸å…³æ€§åˆ†æ•°
        chunk_scores = Counter()
        
        # 1. åŸºäºå…³é”®è¯ç´¢å¼•åŒ¹é…
        for keyword in keywords:
            # ç²¾ç¡®åŒ¹é…
            if keyword in self.keyword_index:
                for chunk_id in self.keyword_index[keyword]:
                    chunk_scores[chunk_id] += 3  # ç´¢å¼•åŒ¹é…ç»™é«˜åˆ†
            
            # æ¨¡ç³ŠåŒ¹é…ï¼ˆéƒ¨åˆ†åŒ¹é…ï¼‰
            for idx, chunk in enumerate(self.chunks):
                content = chunk['content']
                chapter = chunk['chapter']
                
                # å…³é”®è¯åœ¨å†…å®¹ä¸­
                if keyword in content:
                    chunk_scores[chunk['id']] += 2
                
                # å…³é”®è¯åœ¨ç« èŠ‚æ ‡é¢˜ä¸­ï¼ˆæ›´é«˜æƒé‡ï¼‰
                if keyword in chapter:
                    chunk_scores[chunk['id']] += 5
        
        # 2. æ·»åŠ äººç‰©åå’Œåœ°ååŒ¹é…ï¼ˆç‰¹æ®Šå¤„ç†å¸¸è§è§’è‰²åï¼‰
        important_names = [
            'å¾å‡¤å¹´', 'é»„è›®å„¿', 'å¾é¾™è±¡', 'å¾éª', 'åŒ—å‡‰ç‹',
            'å§œæ³¥', 'å—å®«ä»†å°„', 'é™ˆèŠè±¹', 'è¤šç¦„å±±', 'è¢å·¦å®—',
            'ææ·³ç½¡', 'é‚“å¤ªé˜¿', 'æ›¹é•¿å¿', 'ç‹ä»™èŠ',
            'åŒ—å‡‰', 'ç¦»é˜³', 'é¾™è™å±±', 'æ­¦å½“å±±', 'æ­¦å¸åŸ'
        ]
        
        for name in important_names:
            if name in query:
                for chunk in self.chunks:
                    if name in chunk['content'] or name in chunk['chapter']:
                        chunk_scores[chunk['id']] += 4
        
        # 3. åŸºäºæŸ¥è¯¢è¯çš„è¿ç»­åŒ¹é…ï¼ˆ bonusï¼‰
        for chunk in self.chunks:
            # å¦‚æœæŸ¥è¯¢è¯å®Œæ•´å‡ºç°åœ¨å†…å®¹ä¸­ï¼Œç»™é¢å¤–åŠ åˆ†
            if query in chunk['content']:
                chunk_scores[chunk['id']] += 10
        
        # è·å– Top K
        top_chunks = chunk_scores.most_common(top_k)
        
        results = []
        for chunk_id, score in top_chunks:
            if chunk_id in self.chunk_map:
                chunk = self.chunk_map[chunk_id].copy()
                chunk['relevance_score'] = score
                results.append(chunk)
        
        return results
    
    def get_context(self, query: str, top_k: int = 5) -> str:
        """
        è·å–æ£€ç´¢ç»“æœçš„ä¸Šä¸‹æ–‡æ–‡æœ¬ï¼ˆç”¨äº Promptï¼‰
        """
        results = self.retrieve(query, top_k=top_k)
        
        if not results:
            return "æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚"
        
        context_parts = []
        for i, result in enumerate(results, 1):
            context_parts.append(
                f"ã€å‚è€ƒæ®µè½ {i}ã€‘\n"
                f"ç« èŠ‚ï¼š{result['chapter']}\n"
                f"å†…å®¹ï¼š{result['content'][:500]}...\n"  # é™åˆ¶é•¿åº¦é¿å…è¿‡é•¿
                f"ç›¸å…³åº¦ï¼š{result['relevance_score']}\n"
            )
        
        return "\n---\n".join(context_parts)

if __name__ == "__main__":
    retriever = TextRetriever()
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "å¾å‡¤å¹´æ˜¯è°",
        "åŒ—å‡‰ç‹åºœåœ¨å“ªé‡Œ",
        "é»„è›®å„¿æœ‰ä»€ä¹ˆèƒ½åŠ›"
    ]
    
    for query in test_queries:
        print(f"\næŸ¥è¯¢: {query}")
        print("=" * 60)
        results = retriever.retrieve(query, top_k=3)
        for r in results:
            print(f"[{r['chapter']}] (score: {r['relevance_score']})")
            print(f"{r['content'][:200]}...")
            print()
