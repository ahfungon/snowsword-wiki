#!/usr/bin/env python3
"""
å¤šæºç»Ÿä¸€ç´¢å¼•æ¨¡å— - æ”¯æŒå°è¯´åŸæ–‡å’Œè§£è¯´å…¨é›†ç­‰å¤šæºç´¢å¼•
"""

import re
import json
import os
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from collections import defaultdict
import numpy as np


class MultiSourceIndexer:
    """
    å¤šæºç»Ÿä¸€ç´¢å¼•å™¨
    - æ”¯æŒå¤šä¸ªæ–‡æœ¬æºï¼ˆå°è¯´åŸæ–‡ã€è§£è¯´å…¨é›†ç­‰ï¼‰
    - ä¸ºæ¯ä¸ªæ–‡æœ¬å—æ ‡è®°æ¥æº
    - ç»Ÿä¸€æ£€ç´¢æ—¶è€ƒè™‘æ‰€æœ‰æº
    """
    
    def __init__(self, chunk_size: int = 800, overlap: int = 100, api_key: str = None):
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.chunks: List[Dict] = []
        self.keyword_index: Dict = {}
        self.sources: Dict[str, str] = {}  # source_id -> source_name
        
        # å‘é‡ç´¢å¼•ç›¸å…³
        self.api_key = api_key or os.getenv("DEEPSEEK_API_KEY")
    
    def extract_sections(self, text: str, source_name: str) -> List[Tuple[str, str]]:
        """
        æå–ç« èŠ‚æˆ–æ®µè½
        
        é’ˆå¯¹ä¸åŒæ¥æºä½¿ç”¨ä¸åŒçš„æå–ç­–ç•¥ï¼š
        - å°è¯´åŸæ–‡ï¼šæŒ‰ç« èŠ‚æå–
        - è§£è¯´å…¨é›†ï¼šæŒ‰ç« èŠ‚æ ‡é¢˜æå–
        """
        sections = []
        
        if "è§£è¯´" in source_name or "commentary" in source_name.lower():
            # è§£è¯´å…¨é›†æ ¼å¼ï¼šç¬¬ X ç« ï¼šæ ‡é¢˜ + å†…å®¹
            section_pattern = r'(ç¬¬\s*[0-9]+\s*ç« ï¼š[^\n]+)'
            parts = re.split(f'({section_pattern})', text)
            
            current_title = "å‰è¨€"
            current_content = []
            
            for part in parts:
                if not part.strip():
                    continue
                
                if re.match(section_pattern, part.strip()):
                    if current_content:
                        sections.append((current_title, '\n'.join(current_content)))
                    current_title = part.strip()
                    current_content = []
                else:
                    current_content.append(part)
            
            if current_content:
                sections.append((current_title, '\n'.join(current_content)))
        else:
            # å°è¯´åŸæ–‡æ ¼å¼ï¼šç¬¬Xç«  æ ‡é¢˜
            chapter_pattern = r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒé›¶\d]+ç« \s+[^\n]+)'
            parts = re.split(f'({chapter_pattern})', text)
            
            current_title = "åºè¨€"
            current_content = []
            
            for part in parts:
                if not part.strip():
                    continue
                
                if re.match(chapter_pattern, part.strip()):
                    if current_content:
                        sections.append((current_title, '\n'.join(current_content)))
                    current_title = part.strip()
                    current_content = []
                else:
                    current_content.append(part)
            
            if current_content:
                sections.append((current_title, '\n'.join(current_content)))
        
        return sections
    
    def create_chunks(self, text: str, source_name: str, source_id: str) -> List[Dict]:
        """
        å°†æ–‡æœ¬åˆ†å‰²æˆå¸¦å…ƒæ•°æ®çš„å—
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            source_name: æ¥æºåç§°ï¼ˆå¦‚"å°è¯´åŸæ–‡"ã€"è§£è¯´å…¨é›†"ï¼‰
            source_id: æ¥æºIDï¼ˆå¦‚"novel"ã€"commentary"ï¼‰
        """
        print(f"ğŸ“– å¤„ç† {source_name}...")
        sections = self.extract_sections(text, source_name)
        print(f"   æ‰¾åˆ° {len(sections)} ä¸ªç« èŠ‚/æ®µè½")
        
        chunks = []
        base_chunk_id = len(self.chunks)  # åŸºäºå·²æœ‰chunksæ•°é‡
        
        for section_idx, (section_title, section_content) in enumerate(sections):
            content = section_content.strip()
            if not content:
                continue
            
            # æŒ‰å¥å­åˆ†å‰²
            sentences = re.split(r'([ã€‚ï¼ï¼Ÿï¼›\n]+)', content)
            sentences = [s.strip() for s in sentences if s.strip()]
            
            current_chunk = []
            current_size = 0
            
            for i in range(0, len(sentences), 2):
                sentence = sentences[i] if i < len(sentences) else ""
                
                if not sentence:
                    continue
                
                # å¦‚æœè¶…è¿‡å—å¤§å°ï¼Œä¿å­˜å½“å‰å—
                if current_size + len(sentence) > self.chunk_size and current_chunk:
                    chunk_text = ''.join(current_chunk)
                    chunks.append({
                        'id': f"chunk_{base_chunk_id + len(chunks)}",
                        'section': section_title,
                        'content': chunk_text,
                        'char_count': len(chunk_text),
                        'section_idx': section_idx,
                        'source_id': source_id,
                        'source_name': source_name
                    })
                    
                    # ä¿ç•™é‡å éƒ¨åˆ†
                    overlap_text = ''.join(current_chunk[-2:]) if len(current_chunk) >= 2 else chunk_text[-self.overlap:]
                    current_chunk = [overlap_text, sentence]
                    current_size = len(overlap_text) + len(sentence)
                else:
                    current_chunk.append(sentence)
                    current_size += len(sentence)
            
            # ä¿å­˜æœ€åä¸€ä¸ªå—
            if current_chunk:
                chunk_text = ''.join(current_chunk)
                chunks.append({
                    'id': f"chunk_{base_chunk_id + len(chunks)}",
                    'section': section_title,
                    'content': chunk_text,
                    'char_count': len(chunk_text),
                    'section_idx': section_idx,
                    'source_id': source_id,
                    'source_name': source_name
                })
        
        print(f"   å…±åˆ›å»º {len(chunks)} ä¸ªæ–‡æœ¬å—")
        return chunks
    
    def add_source(self, text_path: Path, source_name: str, source_id: str):
        """
        æ·»åŠ ä¸€ä¸ªæ–‡æœ¬æº
        
        Args:
            text_path: æ–‡æœ¬æ–‡ä»¶è·¯å¾„
            source_name: æ¥æºåç§°
            source_id: æ¥æºID
        """
        print(f"\nğŸ“– åŠ è½½ {source_name}: {text_path}")
        
        with open(text_path, 'r', encoding='utf-8') as f:
            text = f.read()
        
        print(f"   æ–‡æœ¬é•¿åº¦: {len(text):,} å­—ç¬¦")
        
        # åˆ›å»ºæ–‡æœ¬å—
        chunks = self.create_chunks(text, source_name, source_id)
        self.chunks.extend(chunks)
        self.sources[source_id] = source_name
        
        print(f"   âœ… {source_name} å¤„ç†å®Œæˆï¼Œå…± {len(chunks)} ä¸ªæ–‡æœ¬å—")
    
    def _build_keyword_index(self) -> Dict:
        """æ„å»ºå…³é”®è¯ç´¢å¼•"""
        try:
            import jieba
        except ImportError:
            print("è­¦å‘Šï¼šæœªå®‰è£… jiebaï¼Œå…³é”®è¯ç´¢å¼•åŠŸèƒ½å—é™")
            return {}
        
        index = defaultdict(list)
        stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'ä¸­', 'ä¸º', 'ä»¥', 'åŠ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œ', 'ä¹‹', 'å…¶', 'æ‰€', 'ç­‰', 'ä¸ª', 'ä»', 'å°†', 'æŠŠ', 'è¢«', 'ç»™', 'è®©', 'å‘', 'å¾€', 'äº', 'å½“', 'ä¸', 'å’Œ', 'è·Ÿ', 'åŒ', 'ç»™', 'ä¸º', 'ä¸ºäº†', 'å› ä¸º', 'æ‰€ä»¥', 'å› æ­¤', 'å¦‚æœ', 'å³ä½¿', 'è™½ç„¶', 'å°½ç®¡', 'ä½†æ˜¯', 'ç„¶è€Œ', 'å¯æ˜¯', 'ä¸è¿‡', 'ç„¶å', 'æ¥ç€', 'äºæ˜¯', 'å°±', 'ä¾¿', 'æ‰', 'å´', 'ç«Ÿ', 'éš¾é“'}
        
        for chunk in self.chunks:
            words = jieba.lcut(chunk['content'])
            
            for word in words:
                word = word.strip()
                if len(word) >= 2 and word not in stop_words:
                    index[word].append(chunk['id'])
        
        for word in index:
            index[word] = list(set(index[word]))
        
        return dict(index)
    
    def build_all_indexes(self, output_dir: str = "data", build_keyword: bool = True):
        """
        æ„å»ºæ‰€æœ‰ç´¢å¼•
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            build_keyword: æ˜¯å¦æ„å»ºå…³é”®è¯ç´¢å¼•
        """
        print(f"\n{'='*60}")
        print("ğŸ”§ æ„å»ºå¤šæºç´¢å¼•")
        print(f"{'='*60}")
        
        print(f"\nğŸ“Š ç»Ÿè®¡:")
        print(f"   æ€»æ–‡æœ¬å—: {len(self.chunks)}")
        for source_id, source_name in self.sources.items():
            count = sum(1 for c in self.chunks if c['source_id'] == source_id)
            print(f"   - {source_name}: {count} ä¸ªæ–‡æœ¬å—")
        
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # ä¿å­˜æ‰€æœ‰ chunks
        chunks_path = output_path / "chunks.json"
        with open(chunks_path, 'w', encoding='utf-8') as f:
            json.dump(self.chunks, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ æ–‡æœ¬å—å·²ä¿å­˜: {chunks_path}")
        
        # ä¿å­˜æ¥æºä¿¡æ¯
        sources_path = output_path / "sources.json"
        with open(sources_path, 'w', encoding='utf-8') as f:
            json.dump(self.sources, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ æ¥æºä¿¡æ¯å·²ä¿å­˜: {sources_path}")
        
        # æ„å»ºå…³é”®è¯ç´¢å¼•
        if build_keyword:
            print("\nğŸ” æ„å»ºå…³é”®è¯ç´¢å¼•...")
            self.keyword_index = self._build_keyword_index()
            keyword_path = output_path / "keyword_index.json"
            with open(keyword_path, 'w', encoding='utf-8') as f:
                json.dump(self.keyword_index, f, ensure_ascii=False)
            print(f"ğŸ’¾ å…³é”®è¯ç´¢å¼•å·²ä¿å­˜: {keyword_path}")
        
        print(f"\n{'='*60}")
        print("âœ… å¤šæºç´¢å¼•æ„å»ºå®Œæˆï¼")
        print(f"{'='*60}")
    
    def search_by_keyword(self, keyword: str, source_filter: str = None) -> List[Dict]:
        """
        æŒ‰å…³é”®è¯æœç´¢
        
        Args:
            keyword: å…³é”®è¯
            source_filter: æ¥æºè¿‡æ»¤å™¨ï¼ˆå¦‚"novel"ã€"commentary"ï¼‰
        """
        if not self.keyword_index:
            return []
        
        chunk_ids = self.keyword_index.get(keyword, [])
        results = []
        
        for chunk_id in chunk_ids:
            chunk = next((c for c in self.chunks if c['id'] == chunk_id), None)
            if chunk:
                if source_filter and chunk['source_id'] != source_filter:
                    continue
                results.append(chunk)
        
        return results
    
    def get_chunks_by_source(self, source_id: str) -> List[Dict]:
        """è·å–ç‰¹å®šæ¥æºçš„æ‰€æœ‰æ–‡æœ¬å—"""
        return [c for c in self.chunks if c['source_id'] == source_id]


# ä¿æŒå‘åå…¼å®¹çš„åˆ«å
UnifiedIndexer = MultiSourceIndexer
TextIndexer = MultiSourceIndexer

if __name__ == "__main__":
    # æµ‹è¯•å¤šæºç´¢å¼•
    indexer = MultiSourceIndexer(chunk_size=800, overlap=100)
    
    # æ·»åŠ å°è¯´åŸæ–‡
    novel_path = Path("data/é›ªä¸­æ‚åˆ€è¡Œ.txt")
    if novel_path.exists():
        indexer.add_source(novel_path, "å°è¯´åŸæ–‡", "novel")
    
    # æ·»åŠ è§£è¯´å…¨é›†
    commentary_path = Path("data/é›ªä¸­æ‚åˆ€è¡Œ_è§£è¯´å…¨é›†.txt")
    if commentary_path.exists():
        indexer.add_source(commentary_path, "è§£è¯´å…¨é›†", "commentary")
    
    # æ„å»ºç´¢å¼•
    indexer.build_all_indexes()