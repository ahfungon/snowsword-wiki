#!/usr/bin/env python3
"""
æ–‡æœ¬å¤„ç†å™¨ V2 (è½»é‡ç‰ˆ) - ä¸ä¾èµ–å¤§æ¨¡å‹
ä½¿ç”¨è§„åˆ™ + è‡ªå®šä¹‰è¯å…¸å®ç°NER
"""

import json
import re
from pathlib import Path
from typing import List, Dict, Tuple
from collections import defaultdict
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class TextProcessorV2:
    """
    è½»é‡ç‰ˆæ–‡æœ¬å¤„ç†å™¨
    - æ™ºèƒ½åˆ†å—
    - è§„åˆ™-based NER
    - åœºæ™¯æ£€æµ‹
    - äº‹ä»¶æŠ½å–
    """
    
    def __init__(self):
        # æ ¸å¿ƒäººç‰©è¯å…¸ï¼ˆå¯æ‰©å±•ï¼‰
        self.characters = {
            'å¾å‡¤å¹´', 'å§œæ³¥', 'å¾éª', 'ææ·³ç½¡', 'å—å®«ä»†å°„', 
            'ç‹ä»™èŠ', 'é‚“å¤ªé˜¿', 'æ‹“è·‹è©è¨', 'æ›¹é•¿å¿', 'é™ˆèŠè±¹',
            'è¤šç¦„å±±', 'è¢å·¦å®—', 'é½ç»ƒå', 'å´ç´ ', 'å¾é¾™è±¡',
            'æ¸©å', 'å‘µå‘µå§‘å¨˜', 'å¾å©´', 'éŸ©è²‚å¯º', 'èµµæ¥·',
            'çº¢è–¯', 'é’é¸Ÿ', 'è€é»„', 'é­å”é˜³'
        }
        
        # åœ°ç‚¹è¯å…¸
        self.locations = {
            'åŒ—å‡‰', 'ç¦»é˜³', 'å¤ªå®‰åŸ', 'æ­¦å¸åŸ', 'æ¸…å‡‰å±±', 
            'åŒ—è½', 'å¹¿é™µæ±Ÿ', 'è¥¿æ¥š', 'ä¸¤ç¦…å¯º', 'é¾™è™å±±',
            'å¬æ½®é˜', 'åŒ—å‡‰ç‹åºœ', 'æ­¦å½“å±±', 'å´å®¶å‰‘å†¢'
        }
        
        # æ­¦åŠŸ/åŠ¿åŠ›
        self.techniques = {
            'ä¸¤è¢–é’è›‡', 'å‰‘å¼€å¤©é—¨', 'ä¸€å‰‘ä»™äººè·ª', 'æŒ‡ç„',
            'å¤©è±¡', 'é‡‘åˆš', 'é™†åœ°ç¥ä»™', 'å¤§é»„åº­', 'é¾™è±¡æ³¢è‹¥åŠŸ'
        }
        
        self.organizations = {
            'åŒ—å‡‰ç‹åºœ', 'å¬æ½®é˜', 'å´å®¶å‰‘å†¢', 'ä¸è‰¯äºº',
            'ç¦»é˜³ç‹æœ', 'åŒ—è½çš‡å¸', 'è¥¿æ¥š', 'ä¸¤ç¦…å¯º', 'é¾™è™å±±'
        }
    
    def split_into_paragraphs(self, text: str) -> List[Dict]:
        """å°†æ–‡æœ¬åˆ†å‰²æˆæ®µè½ï¼Œä¿ç•™ç« èŠ‚ç»“æ„"""
        paragraphs = []
        current_chapter = "æœªçŸ¥ç« èŠ‚"
        
        lines = text.split('\n')
        current_para = []
        para_idx = 0
        chapter_idx = 0
        
        for line in lines:
            line = line.strip()
            
            # è¯†åˆ«ç« èŠ‚æ ‡é¢˜
            chapter_match = re.match(r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡é›¶\d]+ç« )\s*(.+)?', line)
            if chapter_match:
                # ä¿å­˜ä¹‹å‰çš„æ®µè½
                if current_para:
                    paragraphs.append({
                        'chapter': current_chapter,
                        'chapter_idx': chapter_idx,
                        'paragraph_idx': para_idx,
                        'content': '\n'.join(current_para),
                        'length': len(''.join(current_para))
                    })
                    para_idx += 1
                    current_para = []
                
                current_chapter = line
                chapter_idx += 1
                continue
            
            # æ”¶é›†æ®µè½å†…å®¹
            if line and len(line) > 5:
                current_para.append(line)
                
                # æ®µè½é•¿åº¦è¶…è¿‡300å­—æˆ–é‡åˆ°ç©ºè¡Œï¼Œä¿å­˜
                if len(''.join(current_para)) > 300:
                    paragraphs.append({
                        'chapter': current_chapter,
                        'chapter_idx': chapter_idx,
                        'paragraph_idx': para_idx,
                        'content': '\n'.join(current_para),
                        'length': len(''.join(current_para))
                    })
                    para_idx += 1
                    current_para = []
        
        # ä¿å­˜æœ€åä¸€ä¸ªæ®µè½
        if current_para:
            paragraphs.append({
                'chapter': current_chapter,
                'chapter_idx': chapter_idx,
                'paragraph_idx': para_idx,
                'content': '\n'.join(current_para),
                'length': len(''.join(current_para))
            })
        
        return paragraphs
    
    def extract_entities(self, text: str) -> Dict[str, List[str]]:
        """æå–å‘½åå®ä½“ï¼ˆåŸºäºè¯å…¸åŒ¹é…ï¼‰"""
        entities = {
            'äººç‰©': [],
            'åœ°ç‚¹': [],
            'æ­¦åŠŸ': [],
            'åŠ¿åŠ›': []
        }
        
        # åŒ¹é…äººç‰©
        for char in self.characters:
            if char in text:
                entities['äººç‰©'].append(char)
        
        # åŒ¹é…åœ°ç‚¹
        for loc in self.locations:
            if loc in text:
                entities['åœ°ç‚¹'].append(loc)
        
        # åŒ¹é…æ­¦åŠŸ
        for tech in self.techniques:
            if tech in text:
                entities['æ­¦åŠŸ'].append(tech)
        
        # åŒ¹é…åŠ¿åŠ›
        for org in self.organizations:
            if org in text:
                entities['åŠ¿åŠ›'].append(org)
        
        # å»é‡
        for key in entities:
            entities[key] = list(set(entities[key]))
        
        return entities
    
    def detect_scene_type(self, text: str) -> str:
        """æ£€æµ‹åœºæ™¯ç±»å‹"""
        # æˆ˜æ–—åœºæ™¯
        battle_kw = ['æˆ˜', 'æ–—', 'æ€', 'åˆ€', 'å‰‘', 'æ‹³', 'æŒ', 'æ­»', 'è¡€', 'ä¼¤']
        battle_score = sum(1 for kw in battle_kw if kw in text)
        
        # å¯¹è¯åœºæ™¯
        dialogue_kw = ['è¯´', 'é“', 'é—®', 'ç­”', 'æ›°', '"', '"', 'ã€Œ', 'ã€']
        dialogue_score = sum(1 for kw in dialogue_kw if kw in text)
        
        # å¿ƒç†åœºæ™¯
        mental_kw = ['æƒ³', 'è§‰å¾—', 'æ„Ÿè§‰', 'å¿ƒä¸­', 'æš—é“', 'æš—æƒ³']
        mental_score = sum(1 for kw in mental_kw if kw in text)
        
        scores = {'æˆ˜æ–—': battle_score, 'å¯¹è¯': dialogue_score, 'å¿ƒç†': mental_score, 'å™äº‹': 0}
        
        if max(scores.values()) == 0:
            return 'å™äº‹'
        return max(scores, key=scores.get)
    
    def extract_dialogues(self, text: str) -> List[Dict]:
        """æå–å¯¹è¯å†…å®¹"""
        dialogues = []
        
        # åŒ¹é…å¼•å·å†…å®¹
        patterns = [
            r'[""]([^""]{10,200})[""]',  # ä¸­æ–‡å¼•å·
            r'ã€Œ([^ã€]{10,200})ã€',       # æ—¥å¼å¼•å·
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                # æ‰¾åˆ°è¯´è¯äººï¼ˆç®€å•çš„å°±è¿‘åŒ¹é…ï¼‰
                speaker = None
                before = text[:text.find(match)]
                for char in self.characters:
                    if char in before[-50:]:  # å¾€å‰50å­—æ‰¾
                        speaker = char
                        break
                
                dialogues.append({
                    'speaker': speaker,
                    'content': match,
                    'length': len(match)
                })
        
        return dialogues
    
    def extract_events(self, paragraph: Dict) -> List[Dict]:
        """ä»æ®µè½ä¸­æå–äº‹ä»¶"""
        text = paragraph['content']
        events = []
        entities = self.extract_entities(text)
        scene_type = self.detect_scene_type(text)
        
        # æå–å¯¹è¯äº‹ä»¶
        if scene_type == 'å¯¹è¯':
            dialogues = self.extract_dialogues(text)
            for dia in dialogues:
                if dia['speaker']:
                    events.append({
                        'type': 'å¯¹è¯',
                        'character': dia['speaker'],
                        'content': dia['content'][:100],
                        'chapter': paragraph['chapter'],
                        'chapter_idx': paragraph['chapter_idx'],
                        'paragraph_idx': paragraph['paragraph_idx']
                    })
        
        # æå–æˆ˜æ–—/äº’åŠ¨äº‹ä»¶
        elif scene_type == 'æˆ˜æ–—':
            chars = entities.get('äººç‰©', [])
            if len(chars) >= 2:
                events.append({
                    'type': 'æˆ˜æ–—/äº’åŠ¨',
                    'characters': chars[:3],
                    'location': entities.get('åœ°ç‚¹', ['æœªçŸ¥'])[0] if entities.get('åœ°ç‚¹') else 'æœªçŸ¥',
                    'context': text[:150],
                    'chapter': paragraph['chapter'],
                    'chapter_idx': paragraph['chapter_idx'],
                    'paragraph_idx': paragraph['paragraph_idx']
                })
        
        return events
    
    def process_novel(self, text_path: Path, output_dir: Path):
        """å¤„ç†æ•´æœ¬å°è¯´"""
        logger.info(f"ğŸ“– å¤„ç†å°è¯´: {text_path}")
        
        # è¯»å–
        with open(text_path, 'r', encoding='utf-8') as f:
            text = f.read()
        
        logger.info(f"   æ–‡æœ¬é•¿åº¦: {len(text):,} å­—ç¬¦")
        
        # 1. åˆ†å‰²æ®µè½
        logger.info("âœ‚ï¸  åˆ†å‰²æ®µè½...")
        paragraphs = self.split_into_paragraphs(text)
        
        # 2. æå–ä¿¡æ¯
        logger.info("ğŸ” æå–å®ä½“å’Œäº‹ä»¶...")
        all_entities = []
        all_events = []
        
        for i, para in enumerate(paragraphs):
            if (i + 1) % 1000 == 0:
                logger.info(f"   è¿›åº¦: {i+1}/{len(paragraphs)}")
            
            # å®ä½“
            entities = self.extract_entities(para['content'])
            if any(entities.values()):
                all_entities.append({
                    'paragraph_idx': i,
                    **para,
                    'entities': entities
                })
            
            # äº‹ä»¶
            events = self.extract_events(para)
            all_events.extend(events)
        
        # 3. ä¿å­˜
        output_dir.mkdir(exist_ok=True, parents=True)
        
        with open(output_dir / 'paragraphs_v2.json', 'w', encoding='utf-8') as f:
            json.dump(paragraphs, f, ensure_ascii=False, indent=2)
        
        with open(output_dir / 'entities_v2.json', 'w', encoding='utf-8') as f:
            json.dump(all_entities, f, ensure_ascii=False, indent=2)
        
        with open(output_dir / 'events_v2.json', 'w', encoding='utf-8') as f:
            json.dump(all_events, f, ensure_ascii=False, indent=2)
        
        # 4. ç»Ÿè®¡
        stats = {
            'paragraphs': len(paragraphs),
            'entities_records': len(all_entities),
            'events': len(all_events),
            'character_mentions': sum(len(e['entities'].get('äººç‰©', [])) for e in all_entities),
            'dialogues': len([e for e in all_events if e['type'] == 'å¯¹è¯']),
            'battles': len([e for e in all_events if e['type'] == 'æˆ˜æ–—/äº’åŠ¨'])
        }
        
        with open(output_dir / 'stats_v2.json', 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        logger.info("âœ… å¤„ç†å®Œæˆ:")
        for key, val in stats.items():
            logger.info(f"   {key}: {val:,}")
        
        return stats


if __name__ == "__main__":
    # å¿«é€Ÿæµ‹è¯•
    print("ğŸ§ª æµ‹è¯•è½»é‡ç‰ˆæ–‡æœ¬å¤„ç†å™¨")
    
    processor = TextProcessorV2()
    
    test_text = """
ç¬¬ä¸€ç«  å°äºŒä¸Šé…’

å¾å‡¤å¹´ç«™åœ¨åŒ—å‡‰ç‹åºœé—¨å£ï¼Œæœ›ç€è¿œå¤„çš„æ¸…å‡‰å±±ã€‚

"å°äºŒï¼Œä¸Šé…’ï¼"å¾å‡¤å¹´å¤§å£°å–Šé“ã€‚

å§œæ³¥ä»æ—è¾¹èµ°è¿‡ï¼Œå†·å†·åœ°çœ‹äº†ä»–ä¸€çœ¼ã€‚

å¾å‡¤å¹´ç¬‘é“ï¼š"å°æ³¥äººï¼Œä¸€èµ·å»å–é…’ï¼Ÿ"

å§œæ³¥å“¼äº†ä¸€å£°ï¼Œè½¬èº«å°±èµ°ã€‚

ç¬¬äºŒç«  ç™½ç‹å„¿è„¸

å—å®«ä»†å°„æ¥åˆ°å¬æ½®é˜ï¼Œè¦æ‰¾å¾å‡¤å¹´æ¯”è¯•å‰‘æ³•ã€‚
"""
    
    # æµ‹è¯•
    paragraphs = processor.split_into_paragraphs(test_text)
    print(f"\næ®µè½æ•°: {len(paragraphs)}")
    
    for i, p in enumerate(paragraphs[:3]):
        print(f"\næ®µè½ {i+1}:")
        print(f"  ç« èŠ‚: {p['chapter']}")
        print(f"  å†…å®¹: {p['content'][:50]}...")
        
        entities = processor.extract_entities(p['content'])
        if any(entities.values()):
            print(f"  å®ä½“: {entities}")
        
        scene = processor.detect_scene_type(p['content'])
        print(f"  åœºæ™¯: {scene}")
