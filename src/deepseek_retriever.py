#!/usr/bin/env python3
"""
DeepSeek Embedding è¯­ä¹‰æ£€ç´¢å™¨
ä½¿ç”¨ DeepSeek Embedding API è¿›è¡Œå‘é‡è¯­ä¹‰æ£€ç´¢
"""

import os
import json
import numpy as np
from pathlib import Path
from typing import List, Dict, Optional
from openai import OpenAI
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DeepSeekEmbeddingRetriever:
    """
    DeepSeek Embedding è¯­ä¹‰æ£€ç´¢å™¨
    - ä½¿ç”¨ DeepSeek Embedding API ç¼–ç æ–‡æœ¬
    - å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢
    - è¯­ä¹‰ç†è§£ä¼˜äº TF-IDF
    """
    
    def __init__(self, api_key: str = None):
        self.api_key = api_key or os.getenv("DEEPSEEK_API_KEY")
        if not self.api_key:
            raise ValueError("éœ€è¦æä¾› DeepSeek API Key")
        
        self.client = OpenAI(
            api_key=self.api_key,
            base_url="https://api.deepseek.com"
        )
        self.model = "deepseek-embedding"  # DeepSeek Embedding æ¨¡å‹
        
        self.embeddings = None
        self.paragraphs = []
        self.metadata = []
        self.dimension = None
    
    def _get_embedding(self, texts: List[str]) -> List[List[float]]:
        """è°ƒç”¨ DeepSeek Embedding API"""
        try:
            response = self.client.embeddings.create(
                model=self.model,
                input=texts
            )
            return [item.embedding for item in response.data]
        except Exception as e:
            logger.error(f"âŒ Embedding API é”™è¯¯: {e}")
            raise
    
    def _get_embedding_batch(self, texts: List[str], batch_size: int = 100) -> List[List[float]]:
        """æ‰¹é‡è·å– Embeddingï¼ˆå¸¦åˆ†æ‰¹å¤„ç†ï¼‰"""
        all_embeddings = []
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            logger.info(f"ğŸ”„ ç¼–ç æ‰¹æ¬¡ {i//batch_size + 1}/{(len(texts)-1)//batch_size + 1} ({len(batch)} æ¡)")
            embeddings = self._get_embedding(batch)
            all_embeddings.extend(embeddings)
        return all_embeddings
    
    def build_index(self, paragraphs: List[Dict], output_dir: Path):
        """æ„å»ºå‘é‡ç´¢å¼•"""
        logger.info(f"ğŸ”¨ æ„å»º DeepSeek Embedding ç´¢å¼•ï¼Œå…± {len(paragraphs)} ä¸ªæ®µè½")
        
        # ä¿å­˜æ®µè½æ•°æ®
        self.paragraphs = paragraphs
        self.metadata = [
            {
                'idx': i,
                'chapter': p.get('chapter', ''),
                'chapter_idx': p.get('chapter_idx', 0),
                'paragraph_idx': p.get('paragraph_idx', 0),
            }
            for i, p in enumerate(paragraphs)
        ]
        
        # æå–æ–‡æœ¬
        texts = [p['content'] for p in paragraphs]
        
        # è·å– Embeddingï¼ˆåˆ†æ‰¹å¤„ç†ï¼‰
        logger.info("ğŸ”„ è°ƒç”¨ DeepSeek Embedding API...")
        embeddings_list = self._get_embedding_batch(texts)
        self.embeddings = np.array(embeddings_list)
        self.dimension = self.embeddings.shape[1]
        
        logger.info(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆ: {self.embeddings.shape}")
        
        # ä¿å­˜
        output_dir.mkdir(exist_ok=True, parents=True)
        
        # ä¿å­˜å‘é‡
        np.save(output_dir / 'embeddings.npy', self.embeddings)
        
        # ä¿å­˜å…ƒæ•°æ®
        with open(output_dir / 'metadata.json', 'w', encoding='utf-8') as f:
            json.dump({
                'paragraphs': texts,
                'metadata': self.metadata,
                'dimension': self.dimension,
                'model': self.model
            }, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ ç´¢å¼•å·²ä¿å­˜: {output_dir}")
    
    def load_index(self, index_dir: Path):
        """åŠ è½½ç´¢å¼•"""
        logger.info(f"ğŸ“‚ åŠ è½½ DeepSeek Embedding ç´¢å¼•: {index_dir}")
        
        # åŠ è½½å‘é‡
        self.embeddings = np.load(index_dir / 'embeddings.npy')
        self.dimension = self.embeddings.shape[1]
        
        # åŠ è½½å…ƒæ•°æ®
        with open(index_dir / 'metadata.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
            self.paragraphs = data['paragraphs']
            self.metadata = data['metadata']
            saved_model = data.get('model', 'unknown')
        
        logger.info(f"âœ… ç´¢å¼•åŠ è½½å®Œæˆ: {self.embeddings.shape}, æ¨¡å‹: {saved_model}")
    
    def _cosine_similarity(self, query_vec: np.ndarray) -> np.ndarray:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        # å½’ä¸€åŒ–
        query_vec = query_vec / np.linalg.norm(query_vec)
        embeddings_norm = self.embeddings / np.linalg.norm(self.embeddings, axis=1, keepdims=True)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = np.dot(embeddings_norm, query_vec)
        return similarities
    
    def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """è¯­ä¹‰æ£€ç´¢"""
        if self.embeddings is None:
            logger.error("âŒ ç´¢å¼•æœªåŠ è½½")
            return []
        
        # ç¼–ç æŸ¥è¯¢
        logger.info(f"ğŸ” ç¼–ç æŸ¥è¯¢: {query[:50]}...")
        query_embedding = self._get_embedding([query])[0]
        query_vec = np.array(query_embedding)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = self._cosine_similarity(query_vec)
        
        # å– top-k
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        # ç»„è£…ç»“æœ
        results = []
        for idx in top_indices:
            results.append({
                'idx': int(idx),
                'similarity': float(similarities[idx]),
                'content': self.paragraphs[idx],
                **self.metadata[idx]
            })
        
        return results
    
    def hybrid_search(self, query: str, top_k: int = 5, alpha: float = 0.7) -> List[Dict]:
        """æ··åˆæ£€ç´¢ï¼šè¯­ä¹‰ + å…³é”®è¯"""
        import jieba
        
        # è¯­ä¹‰åˆ†æ•°
        query_embedding = self._get_embedding([query])[0]
        query_vec = np.array(query_embedding)
        semantic_scores = self._cosine_similarity(query_vec)
        
        # å…³é”®è¯åˆ†æ•°
        keywords = set(jieba.cut(query))
        keyword_scores = np.zeros(len(self.paragraphs))
        
        for i, content in enumerate(self.paragraphs):
            score = sum(1 for kw in keywords if kw in content)
            keyword_scores[i] = score / max(len(keywords), 1)
        
        # å½’ä¸€åŒ–å…³é”®è¯åˆ†æ•°
        if keyword_scores.max() > 0:
            keyword_scores = keyword_scores / keyword_scores.max()
        
        # èåˆï¼ˆè¯­ä¹‰æƒé‡ alphaï¼Œå…³é”®è¯æƒé‡ 1-alphaï¼‰
        combined_scores = alpha * semantic_scores + (1 - alpha) * keyword_scores
        
        # å– top-k
        top_indices = np.argsort(combined_scores)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            results.append({
                'idx': int(idx),
                'similarity': float(combined_scores[idx]),
                'semantic_score': float(semantic_scores[idx]),
                'keyword_score': float(keyword_scores[idx]),
                'content': self.paragraphs[idx],
                **self.metadata[idx]
            })
        
        return results


if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯• DeepSeek Embedding æ£€ç´¢å™¨")
    print("="*60)
    
    # æµ‹è¯•æ•°æ®
    test_paragraphs = [
        {'content': 'å¾å‡¤å¹´ä¸ºæ¯æŠ¥ä»‡ï¼Œåœ¨å¤ªå®‰åŸå¤–æ–©æ€éŸ©è²‚å¯º', 'chapter': 'ç¬¬100ç« '},
        {'content': 'å§œæ³¥å’Œå¾å‡¤å¹´é’æ¢…ç«¹é©¬ï¼Œæœ€ç»ˆæˆä¸ºåŒ—å‡‰ç‹å¦ƒ', 'chapter': 'ç¬¬200ç« '},
        {'content': 'ææ·³ç½¡ä¼ æˆå¾å‡¤å¹´ä¸¤è¢–é’è›‡ï¼Œå‰‘é“å¤§æˆ', 'chapter': 'ç¬¬50ç« '},
        {'content': 'ç‹ä»™èŠè‡ªç§°å¤©ä¸‹ç¬¬äºŒï¼Œé•‡å®ˆæ­¦å¸åŸä¸€ç”²å­', 'chapter': 'ç¬¬150ç« '},
        {'content': 'å¾éªäººå± ä¹‹åå¨éœ‡å¤©ä¸‹ï¼Œå®ˆæŠ¤åŒ—å‡‰ä¸‰åå¹´', 'chapter': 'ç¬¬10ç« '},
    ]
    
    # åˆ›å»ºæ£€ç´¢å™¨
    api_key = os.getenv("DEEPSEEK_API_KEY")
    if not api_key:
        print("âš ï¸ è¯·è®¾ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")
        exit(1)
    
    retriever = DeepSeekEmbeddingRetriever(api_key=api_key)
    
    # æ„å»ºç´¢å¼•
    retriever.build_index(test_paragraphs, Path("test_index_deepseek"))
    
    # æµ‹è¯•æŸ¥è¯¢
    queries = [
        "å¾å‡¤å¹´ä¸ºä»€ä¹ˆè¦æ€éŸ©è²‚å¯ºï¼Ÿ",
        "å§œæ³¥å’Œå¾å‡¤å¹´çš„å…³ç³»ï¼Ÿ",
        "è°æ•™å¾å‡¤å¹´å‰‘æ³•ï¼Ÿ",
    ]
    
    for query in queries:
        print(f"\nğŸ” æŸ¥è¯¢: {query}")
        results = retriever.search(query, top_k=2)
        for r in results:
            print(f"   [{r['idx']}] {r['content'][:40]}... (ç›¸ä¼¼åº¦: {r['similarity']:.3f})")
