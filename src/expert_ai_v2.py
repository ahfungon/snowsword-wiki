#!/usr/bin/env python3
"""
ä¸“å®¶çº§ AI V2 - æ·±åº¦åˆ†æå›ç­”ç”Ÿæˆå™¨
åŸºäºç°æœ‰çŸ¥è¯†åº“ï¼ˆä¸ä¾èµ–è¯­ä¹‰ç´¢å¼•ï¼‰
"""

import os
import json
from pathlib import Path
from typing import Dict, List, Optional
from openai import OpenAI
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ExpertAIV2:
    """
    ä¸“å®¶çº§ AI V2
    - æ•´åˆçŸ¥è¯†å›¾è°±ã€äº‹ä»¶ã€åŸæ–‡è¿›è¡Œæ·±åº¦åˆ†æ
    - ä¸‰æ®µå¼å›ç­”ï¼šäº‹å®â†’åˆ†æâ†’å‡å
    - æ”¯æŒ DeepSeek Embedding è¯­ä¹‰æ£€ç´¢
    """
    
    def __init__(self, api_key: str = None, knowledge_base_path: Path = None, use_semantic: bool = True):
        self.api_key = api_key or os.getenv("DEEPSEEK_API_KEY")
        self.client = OpenAI(
            api_key=self.api_key,
            base_url="https://api.deepseek.com"
        )
        self.model = "deepseek-chat"
        self.use_semantic = use_semantic  # æ˜¯å¦ä½¿ç”¨è¯­ä¹‰æ£€ç´¢
        
        # åŠ è½½çŸ¥è¯†åº“
        self.knowledge_base = {}
        self.events = []
        self.paragraphs = []
        self.retriever = None  # è¯­ä¹‰æ£€ç´¢å™¨
        
        if knowledge_base_path:
            self._load_knowledge(knowledge_base_path)
    
    def load_semantic_index(self, index_path: Path, zhipu_api_key: str = None):
        """åŠ è½½è¯­ä¹‰æ£€ç´¢ç´¢å¼•ï¼ˆæ™ºè°± AI Embeddingï¼‰"""
        try:
            from .zhipu_retriever import ZhipuEmbeddingRetriever
            
            api_key = zhipu_api_key or os.getenv("ZHIPU_API_KEY")
            if not api_key:
                logger.warning("âš ï¸ æœªæä¾›æ™ºè°± API Keyï¼Œå›é€€åˆ°å…³é”®è¯æ£€ç´¢")
                self.use_semantic = False
                return
            
            self.retriever = ZhipuEmbeddingRetriever(api_key=api_key)
            self.retriever.load_index(index_path)
            self.use_semantic = True
            logger.info("âœ… æ™ºè°±è¯­ä¹‰æ£€ç´¢ç´¢å¼•åŠ è½½æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ è¯­ä¹‰æ£€ç´¢ç´¢å¼•åŠ è½½å¤±è´¥: {e}ï¼Œå›é€€åˆ°å…³é”®è¯æ£€ç´¢")
            self.use_semantic = False
    
    def _load_knowledge(self, base_path: Path):
        """åŠ è½½çŸ¥è¯†åº“æ•°æ®"""
        logger.info("ğŸ“‚ åŠ è½½çŸ¥è¯†åº“...")
        
        # åŠ è½½ä¸“å®¶çŸ¥è¯†åº“
        kb_file = base_path / "expert_knowledge_base.json"
        if kb_file.exists():
            with open(kb_file, 'r', encoding='utf-8') as f:
                self.knowledge_base = json.load(f)
            logger.info(f"   âœ“ çŸ¥è¯†å›¾è°±: {len(self.knowledge_base.get('character_timeline', {}))} äººç‰©")
        
        # åŠ è½½äº‹ä»¶
        events_file = base_path / "processed_v2" / "events_v2.json"
        if events_file.exists():
            with open(events_file, 'r', encoding='utf-8') as f:
                self.events = json.load(f)
            logger.info(f"   âœ“ äº‹ä»¶: {len(self.events)} æ¡")
        
        # åŠ è½½æ®µè½ï¼ˆå‰1000ä¸ªç”¨äºå¿«é€Ÿæ£€ç´¢ï¼‰
        para_file = base_path / "processed_v2" / "paragraphs_v2.json"
        if para_file.exists():
            with open(para_file, 'r', encoding='utf-8') as f:
                all_paras = json.load(f)
                self.paragraphs = all_paras[:2000]  # å…ˆåŠ è½½å‰2000ä¸ª
            logger.info(f"   âœ“ æ®µè½: {len(self.paragraphs)}/{len(all_paras)} (å‰2000)")
    
    def build_system_prompt(self) -> str:
        """ä¸“å®¶çº§ç³»ç»Ÿæç¤ºè¯"""
        return """ä½ æ˜¯ã€Šé›ªä¸­æ‚åˆ€è¡Œã€‹çš„é¡¶å°–æ–‡å­¦è¯„è®ºå®¶å…¼èµ„æ·±ç²¾è¯»è€…ã€‚

å›ç­”åŸåˆ™ï¼š
1. **æ·±åº¦ä¼˜å…ˆäºå¹¿åº¦** - æ·±å…¥åˆ†æ2-3ä¸ªå…³é”®ç‚¹ï¼Œä¸è¦æ³›æ³›è€Œè°ˆ
2. **å› æœé‡äºç½—åˆ—** - è§£é‡Š"ä¸ºä»€ä¹ˆ"æ¯”ç½—åˆ—"å‘ç”Ÿäº†ä»€ä¹ˆ"æ›´é‡è¦
3. **åŸæ–‡ä½œä¸ºè¯æ®** - å…³é”®è®ºç‚¹å¿…é¡»æœ‰åŸæ–‡æ”¯æ’‘ï¼Œå¼•ç”¨è¦ç²¾ç¡®
4. **äººç‰©åŠ¨æœºåˆ†æ** - æ·±å…¥æŒ–æ˜äººç‰©è¡Œä¸ºçš„å†…å¿ƒé©±åŠ¨åŠ›
5. **ä¸»é¢˜å‡å** - ä»å…·ä½“æƒ…èŠ‚ä¸Šå‡åˆ°äººç”Ÿå“²ç†æˆ–ç¤¾ä¼šéšå–»

å›ç­”ç»“æ„ï¼ˆä¸‰æ®µå¼ï¼‰ï¼š
**ã€äº‹å®å±‚ã€‘** ç²¾å‡†å›ç­”å‘ç”Ÿäº†ä»€ä¹ˆï¼ˆäººç‰©ã€äº‹ä»¶ã€ç»“æœï¼‰
**ã€åˆ†æå±‚ã€‘** æ·±å…¥å‰–æåŸå› ã€åŠ¨æœºã€å½±å“ï¼ˆä¸ºä»€ä¹ˆä¼šè¿™æ ·ï¼‰
**ã€å‡åå±‚ã€‘** è”ç³»ä¸»é¢˜ã€è±¡å¾æ„ä¹‰ã€äººç‰©æˆé•¿ï¼ˆæ›´å¤§çš„å›¾æ™¯ï¼‰

è¯­è¨€é£æ ¼ï¼š
- ä¸“ä¸šä½†ä¸æ™¦æ¶©ï¼Œåƒç»™çŸ¥å·±è®²ä¸€ä¸ªæ·±çˆ±çš„æ•…äº‹
- ä½¿ç”¨æ–‡å­¦è¯„è®ºæœ¯è¯­ï¼ˆäººç‰©å¼§å…‰ã€å™äº‹å¼ åŠ›ã€è±¡å¾éšå–»ï¼‰
- å¯ä»¥è¡¨è¾¾å¯¹äººç‰©çš„ç†è§£å’Œæƒ…æ„Ÿå…±é¸£
- å‘ˆç°å¤æ‚æ€§ï¼Œä¸å›é¿ç°è‰²åœ°å¸¦

ç¦æ­¢ï¼š
- æœºæ¢°ç½—åˆ—ç« èŠ‚å†…å®¹
- åªæè¿°ä¸åˆ†æ
- ç»™å‡ºç®€å•çš„é“å¾·åˆ¤æ–­"""
    
    def keyword_search(self, query: str, top_k: int = 5) -> List[Dict]:
        """å…³é”®è¯æ£€ç´¢ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        keywords = set(query.split())
        
        # åœ¨æ®µè½ä¸­æœç´¢
        scored_paras = []
        for para in self.paragraphs:
            content = para['content']
            
            # è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°
            score = sum(2 for kw in keywords if kw in content)
            
            # å¦‚æœæœ‰å®ä½“åŒ¹é…ï¼ŒåŠ åˆ†
            for char in self.knowledge_base.get('character_timeline', {}).keys():
                if char in query and char in content:
                    score += 3
            
            if score > 0:
                scored_paras.append((para, score))
        
        # æ’åºå–å‰K
        scored_paras.sort(key=lambda x: x[1], reverse=True)
        return [p[0] for p in scored_paras[:top_k]]
    
    def semantic_search(self, query: str, top_k: int = 5) -> List[Dict]:
        """è¯­ä¹‰æ£€ç´¢ï¼ˆä½¿ç”¨ DeepSeek Embeddingï¼‰"""
        if self.retriever is None:
            logger.warning("âš ï¸ è¯­ä¹‰æ£€ç´¢å™¨æœªåŠ è½½ï¼Œå›é€€åˆ°å…³é”®è¯æ£€ç´¢")
            return self.keyword_search(query, top_k)
        
        try:
            results = self.retriever.search(query, top_k=top_k)
            # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
            return [
                {
                    'content': r['content'],
                    'chapter': r.get('chapter', 'æœªçŸ¥'),
                    'similarity': r.get('similarity', 0)
                }
                for r in results
            ]
        except Exception as e:
            logger.error(f"âŒ è¯­ä¹‰æ£€ç´¢å¤±è´¥: {e}")
            return self.keyword_search(query, top_k)
    
    def build_context(self, query: str) -> str:
        """æ„å»ºå¢å¼ºä¸Šä¸‹æ–‡"""
        context_parts = []
        
        # 1. æå–æŸ¥è¯¢ä¸­çš„äººç‰©
        mentioned_chars = []
        for char in self.knowledge_base.get('character_timeline', {}).keys():
            if char in query:
                mentioned_chars.append(char)
        
        # 2. æ·»åŠ äººç‰©æ—¶é—´çº¿
        if mentioned_chars:
            context_parts.append("ã€äººç‰©è½¨è¿¹ã€‘")
            for char in mentioned_chars[:2]:
                timeline = self.knowledge_base.get('character_timeline', {}).get(char, [])
                if timeline:
                    # æ‰¾ç›¸å…³äº‹ä»¶
                    relevant = [
                        e for e in timeline 
                        if any(kw in e.get('event', '') for kw in query.split()[:3])
                    ][:3]
                    if not relevant:
                        relevant = timeline[:3]
                    
                    context_parts.append(f"\n{char}:")
                    for e in relevant:
                        context_parts.append(f"  - {e['chapter']}: {e['event'][:80]}...")
        
        # 3. æ·»åŠ æƒ…èŠ‚å› æœï¼ˆå¦‚æœåŒ¹é…ï¼‰
        if self.knowledge_base.get('plot_causes'):
            for event_name, event_data in self.knowledge_base['plot_causes'].items():
                if any(kw in query for kw in event_name.split()):
                    context_parts.append(f"\nã€äº‹ä»¶åˆ†æ: {event_name}ã€‘")
                    context_parts.append(f"åŸå› : {event_data.get('cause', '')}")
                    context_parts.append(f"ç»“æœ: {event_data.get('result', '')}")
                    context_parts.append(f"å½±å“: {event_data.get('impact', '')}")
                    break
        
        # 4. æ·»åŠ åŸæ–‡æ®µè½ï¼ˆä¼˜å…ˆä½¿ç”¨è¯­ä¹‰æ£€ç´¢ï¼‰
        if self.use_semantic and self.retriever is not None:
            logger.info("ğŸ” ä½¿ç”¨è¯­ä¹‰æ£€ç´¢æŸ¥æ‰¾ç›¸å…³åŸæ–‡...")
            relevant_paras = self.semantic_search(query, top_k=3)
            context_parts.append("\nã€ç›¸å…³åŸæ–‡ (è¯­ä¹‰åŒ¹é…)ã€‘")
        else:
            logger.info("ğŸ” ä½¿ç”¨å…³é”®è¯æ£€ç´¢æŸ¥æ‰¾ç›¸å…³åŸæ–‡...")
            relevant_paras = self.keyword_search(query, top_k=3)
            context_parts.append("\nã€ç›¸å…³åŸæ–‡ (å…³é”®è¯åŒ¹é…)ã€‘")
        
        if relevant_paras:
            for i, para in enumerate(relevant_paras, 1):
                context_parts.append(f"\næ®µè½{i} [{para.get('chapter', 'æœªçŸ¥')}]:")
                context_parts.append(para['content'][:300])
        
        return "\n".join(context_parts)
    
    def answer(self, query: str, temperature: float = 0.7) -> Dict:
        """ç”Ÿæˆä¸“å®¶çº§å›ç­”"""
        logger.info(f"ğŸ¤– å¤„ç†é—®é¢˜: {query[:50]}...")
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context = self.build_context(query)
        
        # æ„å»ºæç¤º
        system_prompt = self.build_system_prompt()
        user_prompt = f"""å…³äºã€Šé›ªä¸­æ‚åˆ€è¡Œã€‹çš„æ·±åº¦é—®é¢˜ï¼š

{query}

ä»¥ä¸‹æ˜¯æˆ‘ä¸ºä½ æ•´ç†çš„èƒŒæ™¯ä¿¡æ¯ï¼š

{context}

è¯·ä»¥æ–‡å­¦è¯„è®ºå®¶+ç²¾è¯»è€…çš„èº«ä»½ï¼Œç”¨ä¸‰æ®µå¼ç»“æ„ï¼ˆäº‹å®å±‚â†’åˆ†æå±‚â†’å‡åå±‚ï¼‰å›ç­”è¿™ä¸ªé—®é¢˜ã€‚æ·±å…¥åˆ†æï¼Œä¸è¦æ³›æ³›è€Œè°ˆã€‚"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=temperature,
                max_tokens=2500,
                stream=False
            )
            
            return {
                "success": True,
                "answer": response.choices[0].message.content,
                "query": query,
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                }
            }
        except Exception as e:
            logger.error(f"âŒ API é”™è¯¯: {e}")
            return {
                "success": False,
                "error": str(e),
                "query": query
            }


if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯• Expert AI V2")
    print("="*60)
    
    ai = ExpertAIV2(
        api_key="sk-cdebe0fafcf9406d962e3e09a0404e4b",
        knowledge_base_path=Path("data")
    )
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "å¾å‡¤å¹´ä¸ºä»€ä¹ˆè¦æ€éŸ©è²‚å¯ºï¼Ÿ",
        "ç‹ä»™èŠä¸ºä»€ä¹ˆè‡ªç§°å¤©ä¸‹ç¬¬äºŒï¼Ÿ",
    ]
    
    for query in test_queries:
        print(f"\n{'='*60}")
        print(f"â“ {query}")
        print(f"{'='*60}")
        
        result = ai.answer(query)
        
        if result['success']:
            print(f"\nğŸ’¬ å›ç­”:")
            print(result['answer'])
            print(f"\nğŸ’° Token: {result['usage']['total_tokens']}")
        else:
            print(f"âŒ é”™è¯¯: {result['error']}")
