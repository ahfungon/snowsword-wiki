#!/usr/bin/env python3
"""
ä¸“å®¶æ£€ç´¢å™¨ V2 - å®Œæ•´ç‰ˆ
æ•´åˆæ–‡æœ¬å¤„ç†ã€è¯­ä¹‰æ£€ç´¢ã€çŸ¥è¯†å›¾è°±
"""

import json
import gzip
import subprocess
from pathlib import Path
from typing import List, Dict, Tuple
import logging

from text_processor_v2 import TextProcessorV2
from semantic_retriever_v2 import SemanticRetrieverV2

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ExpertRetrieverV2:
    """
    ä¸“å®¶æ£€ç´¢å™¨ V2
    - æ–‡æœ¬å¤„ç†ï¼ˆæ®µè½åˆ†å‰²ã€å®ä½“æŠ½å–ã€äº‹ä»¶æ£€æµ‹ï¼‰
    - è¯­ä¹‰æ£€ç´¢ï¼ˆBGE Embeddingï¼‰
    - çŸ¥è¯†å›¾è°±ï¼ˆäººç‰©å…³ç³»ã€æƒ…èŠ‚å› æœï¼‰
    """
    
    def __init__(self, data_dir: str = "data"):
        self.data_dir = Path(data_dir)
        self.text_processor = TextProcessorV2()
        self.semantic_retriever = None
        
        # æ•°æ®å­˜å‚¨
        self.paragraphs = []
        self.entities = []
        self.events = []
        self.knowledge_graph = {}
        
        # åŠ è½½æ•°æ®
        self._load_data()
    
    def _load_data(self):
        """åŠ è½½å¤„ç†åçš„æ•°æ®"""
        processed_dir = self.data_dir / "processed_v2"
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å¤„ç†
        if not (processed_dir / "paragraphs_v2.json").exists():
            logger.info("ğŸ“¦ æ•°æ®æœªå¤„ç†ï¼Œå¼€å§‹å¤„ç†...")
            self._process_raw_data()
        
        # åŠ è½½æ®µè½
        logger.info("ğŸ“‚ åŠ è½½æ®µè½æ•°æ®...")
        with open(processed_dir / "paragraphs_v2.json", 'r', encoding='utf-8') as f:
            self.paragraphs = json.load(f)
        
        # åŠ è½½å®ä½“
        if (processed_dir / "entities_v2.json").exists():
            with open(processed_dir / "entities_v2.json", 'r', encoding='utf-8') as f:
                self.entities = json.load(f)
        
        # åŠ è½½äº‹ä»¶
        if (processed_dir / "events_v2.json").exists():
            with open(processed_dir / "events_v2.json", 'r', encoding='utf-8') as f:
                self.events = json.load(f)
        
        # åŠ è½½çŸ¥è¯†å›¾è°±
        kg_path = self.data_dir / "expert_knowledge_base.json"
        if kg_path.exists():
            with open(kg_path, 'r', encoding='utf-8') as f:
                self.knowledge_graph = json.load(f)
        
        logger.info(f"âœ… æ•°æ®åŠ è½½å®Œæˆ:")
        logger.info(f"   æ®µè½: {len(self.paragraphs):,}")
        logger.info(f"   å®ä½“è®°å½•: {len(self.entities):,}")
        logger.info(f"   äº‹ä»¶: {len(self.events):,}")
        if self.knowledge_graph:
            logger.info(f"   äººç‰©: {len(self.knowledge_graph.get('character_timeline', {}))}")
    
    def _process_raw_data(self):
        """å¤„ç†åŸå§‹æ•°æ®"""
        text_file = self.data_dir / "é›ªä¸­æ‚åˆ€è¡Œ.txt"
        if not text_file.exists():
            # è§£å‹
            compressed_files = sorted(self.data_dir.glob("chunks_small_*.gz"))
            if compressed_files:
                logger.info("ğŸ“¦ è§£å‹æ•°æ®...")
                self._decompress_files(compressed_files)
        
        # å¤„ç†
        if text_file.exists():
            self.text_processor.process_novel(
                text_file,
                self.data_dir / "processed_v2"
            )
    
    def _decompress_files(self, compressed_files: List[Path]):
        """è§£å‹æ–‡ä»¶"""
        temp_dir = self.data_dir / "temp_chunks"
        temp_dir.mkdir(exist_ok=True)
        
        logger.info(f"ğŸ—œï¸  è§£å‹ {len(compressed_files)} ä¸ªæ–‡ä»¶...")
        for gz_file in compressed_files:
            output_file = temp_dir / gz_file.stem
            with gzip.open(gz_file, 'rb') as f_in:
                with open(output_file, 'wb') as f_out:
                    f_out.write(f_in.read())
        
        # åˆå¹¶
        output_file = self.data_dir / "é›ªä¸­æ‚åˆ€è¡Œ.txt"
        split_files = sorted(temp_dir.glob("chunks_small_*"))
        files_str = ' '.join([str(f) for f in split_files])
        cmd = f"cat {files_str} > {output_file}"
        subprocess.run(cmd, shell=True, check=True)
        
        # æ¸…ç†
        for f in split_files:
            f.unlink()
        temp_dir.rmdir()
        
        logger.info(f"âœ… è§£å‹å®Œæˆ: {output_file}")
    
    def build_semantic_index(self):
        """æ„å»ºè¯­ä¹‰ç´¢å¼•"""
        logger.info("ğŸ”¨ æ„å»ºè¯­ä¹‰ç´¢å¼•...")
        
        self.semantic_retriever = SemanticRetrieverV2()
        self.semantic_retriever.build_index(
            self.paragraphs,
            self.data_dir / "semantic_index_v2"
        )
        
        logger.info("âœ… è¯­ä¹‰ç´¢å¼•æ„å»ºå®Œæˆ")
    
    def load_semantic_index(self):
        """åŠ è½½è¯­ä¹‰ç´¢å¼•"""
        index_dir = self.data_dir / "semantic_index_v2"
        if not (index_dir / "embeddings.npy").exists():
            logger.info("ğŸ“¦ è¯­ä¹‰ç´¢å¼•ä¸å­˜åœ¨ï¼Œå¼€å§‹æ„å»º...")
            self.build_semantic_index()
        else:
            self.semantic_retriever = SemanticRetrieverV2()
            self.semantic_retriever.load_index(index_dir)
    
    def retrieve(self, query: str, top_k: int = 5) -> List[Dict]:
        """
        æ£€ç´¢ç›¸å…³æ®µè½
        """
        if self.semantic_retriever is None:
            self.load_semantic_index()
        
        # è¯­ä¹‰æ£€ç´¢
        results = self.semantic_retriever.search(query, top_k=top_k)
        
        return results
    
    def get_context(self, query: str, top_k: int = 5) -> str:
        """
        è·å–å¢å¼ºä¸Šä¸‹æ–‡ï¼ˆç”¨äºå›ç­”ç”Ÿæˆï¼‰
        """
        # æ£€ç´¢æ®µè½
        results = self.retrieve(query, top_k=top_k)
        
        # æå–æŸ¥è¯¢ä¸­çš„äººç‰©
        mentioned_chars = []
        for char in self.knowledge_graph.get('character_timeline', {}).keys():
            if char in query:
                mentioned_chars.append(char)
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        
        # 1. äººç‰©æ—¶é—´çº¿ä¿¡æ¯
        if mentioned_chars:
            context_parts.append("ã€äººç‰©è½¨è¿¹ã€‘")
            for char in mentioned_chars[:2]:
                timeline = self.knowledge_graph.get('character_timeline', {}).get(char, [])
                if timeline:
                    # æ‰¾åˆ°ç›¸å…³äº‹ä»¶
                    relevant = [
                        e for e in timeline 
                        if any(kw in query for kw in e.get('event', '')[:20])
                    ][:3]
                    if not relevant:
                        relevant = timeline[:3]
                    
                    context_parts.append(f"\n{char}:")
                    for e in relevant:
                        context_parts.append(f"  - {e['chapter']}: {e['event'][:60]}...")
        
        # 2. ç›¸å…³åŸæ–‡æ®µè½
        context_parts.append("\nã€ç›¸å…³åŸæ–‡ã€‘")
        for i, r in enumerate(results, 1):
            context_parts.append(f"\næ®µè½{i} (ç›¸å…³åº¦: {r['similarity']:.3f}):")
            context_parts.append(r['content'][:300])
        
        return "\n".join(context_parts)


if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯• Expert Retriever V2")
    print("="*60)
    
    # åˆå§‹åŒ–
    retriever = ExpertRetrieverV2()
    
    # æ„å»ºè¯­ä¹‰ç´¢å¼•ï¼ˆå¦‚æœæœªæ„å»ºï¼‰
    # retriever.build_semantic_index()
    
    # æµ‹è¯•æŸ¥è¯¢
    query = "å¾å‡¤å¹´ä¸ºä»€ä¹ˆè¦æ€éŸ©è²‚å¯ºï¼Ÿ"
    print(f"\nğŸ” æŸ¥è¯¢: {query}")
    
    # æ£€ç´¢
    results = retriever.retrieve(query, top_k=3)
    
    print(f"\næ£€ç´¢ç»“æœ:")
    for r in results:
        print(f"  [{r['idx']}] {r['content'][:80]}...")
        print(f"      ç›¸ä¼¼åº¦: {r['similarity']:.3f}, ç« èŠ‚: {r['chapter']}")
