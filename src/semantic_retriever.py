#!/usr/bin/env python3
"""
è¯­ä¹‰æ£€ç´¢å™¨ V2 - åŸºäº BGE ä¸­æ–‡ Embedding
å®ç°çœŸæ­£çš„è¯­ä¹‰ç†è§£æ£€ç´¢
"""

import json
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SemanticRetriever:
    """
    è¯­ä¹‰æ£€ç´¢å™¨
    - ä½¿ç”¨ BGE ä¸­æ–‡ Embedding
    - æ”¯æŒ ChromaDB å‘é‡å­˜å‚¨
    - æ··åˆæ£€ç´¢ï¼ˆè¯­ä¹‰ + å…³é”®è¯ï¼‰
    """
    
    def __init__(self, model_name: str = "BAAI/bge-base-zh", device: str = "cpu"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.collection = None
        self._load_model()
    
    def _load_model(self):
        """åŠ è½½ Embedding æ¨¡å‹"""
        try:
            from sentence_transformers import SentenceTransformer
            logger.info(f"ğŸ“¥ åŠ è½½æ¨¡å‹: {self.model_name}")
            self.model = SentenceTransformer(self.model_name, device=self.device)
            logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def encode(self, texts: List[str]) -> np.ndarray:
        """ç¼–ç æ–‡æœ¬ä¸ºå‘é‡"""
        if isinstance(texts, str):
            texts = [texts]
        
        # BGE æ¨¡å‹éœ€è¦åœ¨æ–‡æœ¬å‰åŠ æŒ‡ä»¤
        instruction = "ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š"
        texts_with_instruction = [f"{instruction}{t}" for t in texts]
        
        embeddings = self.model.encode(
            texts_with_instruction,
            normalize_embeddings=True,
            show_progress_bar=False
        )
        return embeddings
    
    def cosine_similarity(self, query_emb: np.ndarray, doc_emb: np.ndarray) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        return np.dot(query_emb, doc_emb)
    
    def build_index(self, chunks: List[Dict], index_path: Path = None):
        """
        æ„å»ºå‘é‡ç´¢å¼•
        chunks: [{'id': ..., 'content': ..., 'chapter': ...}, ...]
        """
        logger.info(f"ğŸ”¨ æ„å»ºå‘é‡ç´¢å¼•ï¼Œå…± {len(chunks)} ä¸ªæ–‡æœ¬å—")
        
        # æå–æ–‡æœ¬å†…å®¹
        texts = [chunk['content'] for chunk in chunks]
        ids = [str(chunk['id']) for chunk in chunks]
        metadatas = [
            {
                'chapter': chunk.get('chapter', ''),
                'chapter_idx': chunk.get('chapter_idx', 0)
            }
            for chunk in chunks
        ]
        
        # ç¼–ç 
        logger.info("ğŸ”„ ç¼–ç æ–‡æœ¬...")
        embeddings = self.model.encode(
            texts,
            normalize_embeddings=True,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        
        # ä½¿ç”¨ ChromaDB å­˜å‚¨
        try:
            import chromadb
            client = chromadb.Client()
            
            # åˆ›å»ºæˆ–è·å–é›†åˆ
            self.collection = client.create_collection(
                name="snowsword",
                metadata={"hnsw:space": "cosine"}
            )
            
            # åˆ†æ‰¹æ·»åŠ ï¼ˆé¿å…å†…å­˜é—®é¢˜ï¼‰
            batch_size = 1000
            for i in range(0, len(chunks), batch_size):
                batch_end = min(i + batch_size, len(chunks))
                self.collection.add(
                    embeddings=embeddings[i:batch_end].tolist(),
                    documents=texts[i:batch_end],
                    metadatas=metadatas[i:batch_end],
                    ids=ids[i:batch_end]
                )
                logger.info(f"   å·²æ·»åŠ : {batch_end}/{len(chunks)}")
            
            logger.info("âœ… å‘é‡ç´¢å¼•æ„å»ºå®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ ChromaDB é”™è¯¯: {e}")
            # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ numpy å­˜å‚¨
            self._save_numpy_index(embeddings, ids, metadatas, index_path)
    
    def _save_numpy_index(self, embeddings: np.ndarray, ids: List[str], 
                         metadatas: List[Dict], index_path: Path):
        """å¤‡ç”¨ï¼šä½¿ç”¨ numpy å­˜å‚¨ç´¢å¼•"""
        if index_path is None:
            index_path = Path("data/semantic_index")
        
        index_path.mkdir(exist_ok=True, parents=True)
        
        np.save(index_path / "embeddings.npy", embeddings)
        with open(index_path / "metadata.json", 'w', encoding='utf-8') as f:
            json.dump({
                'ids': ids,
                'metadatas': metadatas
            }, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… Numpy ç´¢å¼•å·²ä¿å­˜: {index_path}")
    
    def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """
        è¯­ä¹‰æ£€ç´¢
        """
        if self.collection is None:
            logger.error("âŒ ç´¢å¼•æœªåŠ è½½")
            return []
        
        # ç¼–ç æŸ¥è¯¢
        query_embedding = self.encode([query])
        
        # æ£€ç´¢
        results = self.collection.query(
            query_embeddings=query_embedding.tolist(),
            n_results=top_k
        )
        
        # æ ¼å¼åŒ–ç»“æœ
        formatted_results = []
        for i in range(len(results['ids'][0])):
            formatted_results.append({
                'id': results['ids'][0][i],
                'content': results['documents'][0][i],
                'metadata': results['metadatas'][0][i],
                'distance': results['distances'][0][i]
            })
        
        return formatted_results
    
    def hybrid_search(self, query: str, keyword_index: Dict, top_k: int = 5) -> List[Dict]:
        """
        æ··åˆæ£€ç´¢ï¼šè¯­ä¹‰ + å…³é”®è¯
        """
        # è¯­ä¹‰æ£€ç´¢
        semantic_results = self.search(query, top_k=top_k * 2)
        
        # å…³é”®è¯æ£€ç´¢ï¼ˆç®€å•å®ç°ï¼‰
        keywords = set(query.split())
        keyword_scores = {}
        
        for chunk_id, chunk_data in keyword_index.items():
            content = chunk_data.get('content', '')
            score = sum(1 for kw in keywords if kw in content)
            if score > 0:
                keyword_scores[chunk_id] = score
        
        # èåˆæ’åº
        fused_scores = {}
        
        # è¯­ä¹‰åˆ†æ•°ï¼ˆå½’ä¸€åŒ–åˆ°0-1ï¼‰
        for i, result in enumerate(semantic_results):
            fused_scores[result['id']] = fused_scores.get(result['id'], 0) + (1 - i / len(semantic_results)) * 0.6
        
        # å…³é”®è¯åˆ†æ•°
        max_kw_score = max(keyword_scores.values()) if keyword_scores else 1
        for chunk_id, score in keyword_scores.items():
            fused_scores[chunk_id] = fused_scores.get(chunk_id, 0) + (score / max_kw_score) * 0.4
        
        # æ’åº
        sorted_ids = sorted(fused_scores.keys(), key=lambda x: fused_scores[x], reverse=True)
        
        # ç»„è£…ç»“æœ
        final_results = []
        for chunk_id in sorted_ids[:top_k]:
            # ä»è¯­ä¹‰ç»“æœä¸­æ‰¾
            found = False
            for result in semantic_results:
                if result['id'] == chunk_id:
                    final_results.append(result)
                    found = True
                    break
            
            # å¦‚æœä¸åœ¨è¯­ä¹‰ç»“æœä¸­ï¼Œä»å…³é”®è¯ç´¢å¼•æ‰¾
            if not found and chunk_id in keyword_index:
                final_results.append({
                    'id': chunk_id,
                    'content': keyword_index[chunk_id]['content'],
                    'metadata': keyword_index[chunk_id].get('metadata', {}),
                    'distance': 0.5  # é»˜è®¤ä¸­ç­‰ç›¸å…³åº¦
                })
        
        return final_results


if __name__ == "__main__":
    # æµ‹è¯•
    print("ğŸ§ª æµ‹è¯•è¯­ä¹‰æ£€ç´¢å™¨")
    
    # åˆ›å»ºæ£€ç´¢å™¨
    retriever = SemanticRetriever()
    
    # æµ‹è¯•æ•°æ®
    test_chunks = [
        {'id': '1', 'content': 'å¾å‡¤å¹´ä¸ºæ¯æŠ¥ä»‡ï¼Œæ–©æ€éŸ©è²‚å¯ºäºå¤ªå®‰åŸå¤–', 'chapter': 'ç¬¬100ç« '},
        {'id': '2', 'content': 'å§œæ³¥å’Œå¾å‡¤å¹´é’æ¢…ç«¹é©¬ï¼Œæœ€ç»ˆæˆä¸ºåŒ—å‡‰ç‹å¦ƒ', 'chapter': 'ç¬¬200ç« '},
        {'id': '3', 'content': 'ææ·³ç½¡ä¼ æˆä¸¤è¢–é’è›‡ï¼Œå¾å‡¤å¹´å‰‘é“å¤§æˆ', 'chapter': 'ç¬¬50ç« '},
    ]
    
    # æ„å»ºç´¢å¼•
    retriever.build_index(test_chunks)
    
    # æµ‹è¯•æŸ¥è¯¢
    queries = [
        "å¾å‡¤å¹´ä¸ºä»€ä¹ˆè¦æ€éŸ©è²‚å¯ºï¼Ÿ",
        "å§œæ³¥å’Œå¾å‡¤å¹´ä»€ä¹ˆå…³ç³»ï¼Ÿ",
    ]
    
    for query in queries:
        print(f"\nğŸ” æŸ¥è¯¢: {query}")
        results = retriever.search(query, top_k=2)
        for r in results:
            print(f"   [{r['id']}] {r['content'][:50]}... (ç›¸ä¼¼åº¦: {1-r['distance']:.3f})")
