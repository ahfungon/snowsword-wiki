#!/usr/bin/env python3
"""
ä¸“å®¶ç³»ç»Ÿ V2 å®Œæ•´ç‰ˆ - æ”¯æŒæ™ºè°±è¯­ä¹‰æ£€ç´¢
æ•´åˆï¼šæ–‡æœ¬å¤„ç† + è¯­ä¹‰æ£€ç´¢ + ä¸“å®¶AI
"""

import json
import os
from pathlib import Path
from typing import List, Dict
import logging
import sys

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# æ·»åŠ  src åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from lightweight_retriever import LightweightRetriever
from expert_ai_v2 import ExpertAIV2

# å°è¯•å¯¼å…¥æ™ºè°±æ£€ç´¢å™¨
try:
    from zhipu_retriever import ZhipuEmbeddingRetriever
    ZHIPU_RETRIEVER_AVAILABLE = True
    logger.info("âœ… ZhipuEmbeddingRetriever å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    logger.warning(f"âš ï¸ æ— æ³•å¯¼å…¥ ZhipuEmbeddingRetriever: {e}")
    ZhipuEmbeddingRetriever = None
    ZHIPU_RETRIEVER_AVAILABLE = False


class ExpertSystemV2:
    """
    ä¸“å®¶ç³»ç»Ÿ V2ï¼ˆæ”¯æŒæ™ºè°±è¯­ä¹‰æ£€ç´¢ï¼‰
    """
    
    def __init__(self, data_dir: str = "data", api_key: str = None, zhipu_api_key: str = None):
        self.data_dir = Path(data_dir)
        self.zhipu_api_key = zhipu_api_key or os.getenv("ZHIPU_API_KEY")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.retriever = None
        self.ai = None
        self.use_semantic = False  # æ˜¯å¦ä½¿ç”¨è¯­ä¹‰æ£€ç´¢
        
        # åŠ è½½
        self._init_retriever()
        self._init_ai(api_key)
    
    def _init_retriever(self):
        """åˆå§‹åŒ–æ£€ç´¢å™¨ï¼ˆä¼˜å…ˆä½¿ç”¨æ™ºè°±è¯­ä¹‰ç´¢å¼•ï¼‰"""
        logger.info("ğŸ“‚ åˆå§‹åŒ–æ£€ç´¢å™¨...")
        
        # 1. ä¼˜å…ˆå°è¯•åŠ è½½æ™ºè°±è¯­ä¹‰ç´¢å¼•
        zhipu_index_dir = self.data_dir / "zhipu_index"
        if zhipu_index_dir.exists() and (zhipu_index_dir / "embeddings.npy").exists():
            if self.zhipu_api_key:
                try:
                    if not ZHIPU_RETRIEVER_AVAILABLE or ZhipuEmbeddingRetriever is None:
                        raise ImportError("ZhipuEmbeddingRetriever æ¨¡å—ä¸å¯ç”¨")
                    self.retriever = ZhipuEmbeddingRetriever(api_key=self.zhipu_api_key)
                    self.retriever.load_index(zhipu_index_dir)
                    self.use_semantic = True
                    logger.info("âœ… æ™ºè°±è¯­ä¹‰æ£€ç´¢å™¨åŠ è½½æˆåŠŸ")
                    return
                except Exception as e:
                    logger.warning(f"âš ï¸ æ™ºè°±è¯­ä¹‰ç´¢å¼•åŠ è½½å¤±è´¥: {e}")
            else:
                logger.info("â„¹ï¸ æœªé…ç½® ZHIPU_API_KEYï¼Œè·³è¿‡è¯­ä¹‰ç´¢å¼•")
        
        # 2. å›é€€åˆ° TF-IDF ç´¢å¼•
        logger.info("ğŸ“‚ ä½¿ç”¨ TF-IDF ç´¢å¼•...")
        index_dir = self.data_dir / "semantic_index_light"
        
        # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
        if not (index_dir / "tfidf_matrix.npz").exists():
            logger.info("ğŸ“¦ ç´¢å¼•ä¸å­˜åœ¨ï¼Œå¼€å§‹æ„å»º...")
            self._build_index()
        
        # åŠ è½½ç´¢å¼•
        self.retriever = LightweightRetriever()
        self.retriever.load_index(index_dir)
        self.use_semantic = False
        
        logger.info("âœ… TF-IDF æ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _build_index(self):
        """æ„å»º TF-IDF ç´¢å¼•ï¼ˆå¤‡ç”¨ï¼‰"""
        from text_processor_v2 import TextProcessorV2
        
        # æ£€æŸ¥å¤„ç†åçš„æ•°æ®æ˜¯å¦å­˜åœ¨
        para_file = self.data_dir / "processed_v2" / "paragraphs_v2.json"
        
        if not para_file.exists():
            logger.info("ğŸ“¦ å¤„ç†åŸå§‹æ•°æ®...")
            processor = TextProcessorV2()
            processor.process_novel(
                self.data_dir / "é›ªä¸­æ‚åˆ€è¡Œ.txt",
                self.data_dir / "processed_v2"
            )
        
        # åŠ è½½æ®µè½å¹¶æ„å»ºç´¢å¼•
        logger.info("ğŸ”¨ æ„å»º TF-IDF ç´¢å¼•...")
        with open(para_file, 'r', encoding='utf-8') as f:
            paragraphs = json.load(f)
        
        retriever = LightweightRetriever()
        retriever.build_index(paragraphs, self.data_dir / "semantic_index_light")
    
    def _init_ai(self, api_key: str = None):
        """åˆå§‹åŒ–AI"""
        logger.info("ğŸ¤– åˆå§‹åŒ–ä¸“å®¶AI...")
        
        self.ai = ExpertAIV2(
            api_key=api_key,
            knowledge_base_path=self.data_dir
        )
        
        # å¦‚æœç”¨äº†æ™ºè°±æ£€ç´¢å™¨ï¼Œä¹ŸåŠ è½½åˆ° AI ä¸­
        if self.use_semantic and isinstance(self.retriever, ZhipuEmbeddingRetriever):
            self.ai.retriever = self.retriever
            self.ai.use_semantic = True
            logger.info("âœ… AI å·²å…³è”è¯­ä¹‰æ£€ç´¢å™¨")
        
        logger.info("âœ… AIåˆå§‹åŒ–å®Œæˆ")
    
    def retrieve(self, query: str, top_k: int = 5) -> List[Dict]:
        """æ£€ç´¢ç›¸å…³æ®µè½"""
        return self.retriever.search(query, top_k=top_k)
    
    def get_context(self, query: str, top_k: int = 5) -> str:
        """è·å–å¢å¼ºä¸Šä¸‹æ–‡"""
        # æ£€ç´¢æ®µè½
        results = self.retrieve(query, top_k=top_k)
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        
        # 1. ç›¸å…³åŸæ–‡æ®µè½
        index_type = "è¯­ä¹‰åŒ¹é…" if self.use_semantic else "å…³é”®è¯åŒ¹é…"
        context_parts.append(f"ã€ç›¸å…³åŸæ–‡ ({index_type})ã€‘")
        for i, r in enumerate(results, 1):
            context_parts.append(f"\næ®µè½{i} [{r.get('chapter', 'æœªçŸ¥')}] (ç›¸å…³åº¦: {r.get('similarity', 0):.3f}):")
            context_parts.append(r['content'][:400])
        
        # 2. çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼ˆå¦‚æœAIå·²åŠ è½½ï¼‰
        if self.ai and self.ai.knowledge_base:
            # æå–æŸ¥è¯¢ä¸­çš„äººç‰©
            mentioned_chars = []
            for char in self.ai.knowledge_base.get('character_timeline', {}).keys():
                if char in query:
                    mentioned_chars.append(char)
            
            if mentioned_chars:
                context_parts.append("\n\nã€äººç‰©èƒŒæ™¯ã€‘")
                for char in mentioned_chars[:2]:
                    timeline = self.ai.knowledge_base.get('character_timeline', {}).get(char, [])
                    if timeline:
                        relevant = [
                            e for e in timeline 
                            if any(kw in e.get('event', '') for kw in query.split()[:3])
                        ][:2]
                        if relevant:
                            context_parts.append(f"\n{char}:")
                            for e in relevant:
                                context_parts.append(f"  - {e['chapter']}: {e['event'][:60]}...")
        
        return "\n".join(context_parts)
    
    def answer(self, query: str, temperature: float = 0.7) -> Dict:
        """ç”Ÿæˆä¸“å®¶çº§å›ç­”"""
        logger.info(f"ğŸ¤– å¤„ç†é—®é¢˜: {query[:50]}...")
        
        # è·å–ä¸Šä¸‹æ–‡
        context = self.get_context(query)
        
        # æ„å»ºæç¤º
        system_prompt = self.ai.build_system_prompt()
        user_prompt = f"""å…³äºã€Šé›ªä¸­æ‚åˆ€è¡Œã€‹çš„æ·±åº¦é—®é¢˜ï¼š

{query}

ä»¥ä¸‹æ˜¯æˆ‘ä¸ºä½ æ•´ç†çš„èƒŒæ™¯ä¿¡æ¯ï¼ˆåŒ…æ‹¬ç›¸å…³åŸæ–‡å’Œäººç‰©èƒŒæ™¯ï¼‰ï¼š

{context}

è¯·ä»¥æ–‡å­¦è¯„è®ºå®¶+ç²¾è¯»è€…çš„èº«ä»½ï¼Œç”¨ä¸‰æ®µå¼ç»“æ„ï¼ˆäº‹å®å±‚â†’åˆ†æå±‚â†’å‡åå±‚ï¼‰å›ç­”è¿™ä¸ªé—®é¢˜ã€‚æ·±å…¥åˆ†æï¼Œä¸è¦æ³›æ³›è€Œè°ˆã€‚"""
        
        # è°ƒç”¨API
        try:
            response = self.ai.client.chat.completions.create(
                model=self.ai.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=temperature,
                max_tokens=2500,
                stream=False
            )
            
            return {
                "success": True,
                "answer": response.choices[0].message.content,
                "query": query,
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                },
                "retrieval_mode": "semantic" if self.use_semantic else "tfidf"
            }
        except Exception as e:
            logger.error(f"âŒ API é”™è¯¯: {e}")
            return {
                "success": False,
                "error": str(e),
                "query": query
            }


if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯• Expert System V2")
    print("="*60)
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    system = ExpertSystemV2(
        data_dir="data",
        api_key="sk-cdebe0fafcf9406d962e3e09a0404e4b"
    )
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "å¾å‡¤å¹´ä¸ºä»€ä¹ˆè¦æ€éŸ©è²‚å¯ºï¼Ÿ",
        "ç‹ä»™èŠä¸ºä»€ä¹ˆè‡ªç§°å¤©ä¸‹ç¬¬äºŒï¼Ÿ",
    ]
    
    for query in test_queries:
        print(f"\n{'='*60}")
        print(f"â“ {query}")
        print(f"{'='*60}")
        
        result = system.answer(query)
        
        if result['success']:
            print(f"\nğŸ’¬ å›ç­”:")
            print(result['answer'])
            print(f"\nğŸ’° Token: {result['usage']['total_tokens']}")
            print(f"ğŸ” æ£€ç´¢æ¨¡å¼: {result.get('retrieval_mode', 'unknown')}")
        else:
            print(f"âŒ é”™è¯¯: {result['error']}")
