"""
ç»Ÿä¸€ç´¢å¼•æ¨¡å— - æ•´åˆå…³é”®è¯ç´¢å¼•å’Œå‘é‡ç´¢å¼•
"""

import re
import json
import os
import pickle
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from collections import defaultdict
import numpy as np
from openai import OpenAI


class UnifiedIndexer:
    """ç»Ÿä¸€ç´¢å¼•å™¨ï¼šæ”¯æŒå…³é”®è¯ç´¢å¼•å’Œå‘é‡ç´¢å¼•"""
    
    def __init__(self, chunk_size: int = 800, overlap: int = 100, api_key: str = None):
        """
        åˆå§‹åŒ–ç´¢å¼•å™¨
        
        Args:
            chunk_size: æ¯ä¸ªæ–‡æœ¬å—çš„å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            overlap: ç›¸é‚»å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°
            api_key: DeepSeek/OpenAI API Keyï¼ˆç”¨äºå‘é‡ç´¢å¼•ï¼‰
        """
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.chunks: List[Dict] = []
        self.keyword_index: Dict = {}
        
        # å‘é‡ç´¢å¼•ç›¸å…³
        self.api_key = api_key or os.getenv("DEEPSEEK_API_KEY")
        if self.api_key:
            self.client = OpenAI(
                api_key=self.api_key,
                base_url="https://api.deepseek.com"
            )
        else:
            self.client = None
    
    def extract_chapters(self, text: str) -> List[Tuple[str, str]]:
        """æå–ç« èŠ‚æ ‡é¢˜å’Œå†…å®¹"""
        chapter_pattern = r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒé›¶\d]+ç« \s+[^\n]+)'
        parts = re.split(f'({chapter_pattern})', text)
        
        chapters = []
        current_title = "åºè¨€"
        current_content = []
        
        for part in parts:
            if not part.strip():
                continue
            
            if re.match(chapter_pattern, part.strip()):
                if current_content:
                    chapters.append((current_title, '\n'.join(current_content)))
                current_title = part.strip()
                current_content = []
            else:
                current_content.append(part)
        
        if current_content:
            chapters.append((current_title, '\n'.join(current_content)))
        
        return chapters
    
    def create_chunks(self, text: str) -> List[Dict]:
        """å°†æ–‡æœ¬åˆ†å‰²æˆå¸¦å…ƒæ•°æ®çš„å—"""
        print("æ­£åœ¨æå–ç« èŠ‚...")
        chapters = self.extract_chapters(text)
        print(f"æ‰¾åˆ° {len(chapters)} ä¸ªç« èŠ‚")
        
        chunks = []
        
        for chapter_idx, (chapter_title, chapter_content) in enumerate(chapters):
            content = chapter_content.strip()
            if not content:
                continue
            
            sentences = re.split(r'([ã€‚ï¼ï¼Ÿï¼›\n]+)', content)
            sentences = [s.strip() for s in sentences if s.strip()]
            
            current_chunk = []
            current_size = 0
            
            for i in range(0, len(sentences), 2):
                sentence = sentences[i] if i < len(sentences) else ""
                
                if not sentence:
                    continue
                
                if current_size + len(sentence) > self.chunk_size and current_chunk:
                    chunk_text = ''.join(current_chunk)
                    chunks.append({
                        'id': f"chunk_{len(chunks)}",
                        'chapter': chapter_title,
                        'content': chunk_text,
                        'char_count': len(chunk_text),
                        'chapter_idx': chapter_idx
                    })
                    
                    overlap_text = ''.join(current_chunk[-2:]) if len(current_chunk) >= 2 else chunk_text[-self.overlap:]
                    current_chunk = [overlap_text, sentence]
                    current_size = len(overlap_text) + len(sentence)
                else:
                    current_chunk.append(sentence)
                    current_size += len(sentence)
            
            if current_chunk:
                chunk_text = ''.join(current_chunk)
                chunks.append({
                    'id': f"chunk_{len(chunks)}",
                    'chapter': chapter_title,
                    'content': chunk_text,
                    'char_count': len(chunk_text),
                    'chapter_idx': chapter_idx
                })
        
        print(f"å…±åˆ›å»º {len(chunks)} ä¸ªæ–‡æœ¬å—")
        return chunks
    
    def _build_keyword_index(self) -> Dict:
        """æ„å»ºç®€å•å…³é”®è¯ç´¢å¼•"""
        try:
            import jieba
        except ImportError:
            print("è­¦å‘Šï¼šæœªå®‰è£… jiebaï¼Œå…³é”®è¯ç´¢å¼•åŠŸèƒ½å—é™")
            return {}
        
        index = defaultdict(list)
        stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'}
        
        for chunk in self.chunks:
            words = jieba.lcut(chunk['content'])
            
            for word in words:
                word = word.strip()
                if len(word) >= 2 and word not in stop_words:
                    index[word].append(chunk['id'])
        
        for word in index:
            index[word] = list(set(index[word]))
        
        return dict(index)
    
    def _get_embedding(self, text: str) -> List[float]:
        """è·å–æ–‡æœ¬çš„å‘é‡åµŒå…¥ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        words = set(text[:200])
        vector = []
        for char in words:
            vector.append(ord(char) % 100 / 100.0)
        
        while len(vector) < 128:
            vector.append(0.0)
        return vector[:128]
    
    def _create_vector_index(self) -> List[Dict]:
        """ä¸ºæ‰€æœ‰æ–‡æœ¬å—ç”Ÿæˆå‘é‡ç´¢å¼•"""
        print(f"ğŸ§® æ­£åœ¨ä¸º {len(self.chunks)} ä¸ªæ–‡æœ¬å—ç”Ÿæˆå‘é‡åµŒå…¥...")
        
        indexed_chunks = []
        for i, chunk in enumerate(self.chunks):
            if (i + 1) % 1000 == 0:
                print(f"  è¿›åº¦: {i+1}/{len(self.chunks)}")
            
            vector = self._get_embedding(chunk['content'])
            indexed_chunks.append({**chunk, 'embedding': vector})
        
        print(f"âœ… å‘é‡ç´¢å¼•ç”Ÿæˆå®Œæˆ")
        return indexed_chunks
    
    def build_index(self, text_path: str, output_dir: str = "data", 
                    build_keyword: bool = True, build_vector: bool = True):
        """
        æ„å»ºå®Œæ•´ç´¢å¼•
        
        Args:
            text_path: æ–‡æœ¬æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            build_keyword: æ˜¯å¦æ„å»ºå…³é”®è¯ç´¢å¼•
            build_vector: æ˜¯å¦æ„å»ºå‘é‡ç´¢å¼•
        """
        print(f"ğŸ“– è¯»å–æ–‡æœ¬æ–‡ä»¶: {text_path}")
        with open(text_path, 'r', encoding='utf-8') as f:
            text = f.read()
        
        print(f"ğŸ“ æ–‡æœ¬æ€»é•¿åº¦: {len(text)} å­—ç¬¦")
        
        # åˆ›å»ºæ–‡æœ¬å—
        self.chunks = self.create_chunks(text)
        
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # ä¿å­˜åŸºç¡€ chunks
        json_path = output_path / "chunks.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(self.chunks, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ æ–‡æœ¬å—å·²ä¿å­˜: {json_path}")
        
        # æ„å»ºå…³é”®è¯ç´¢å¼•
        if build_keyword:
            print("ğŸ” æ„å»ºå…³é”®è¯ç´¢å¼•...")
            self.keyword_index = self._build_keyword_index()
            keyword_path = output_path / "keyword_index.json"
            with open(keyword_path, 'w', encoding='utf-8') as f:
                json.dump(self.keyword_index, f, ensure_ascii=False)
            print(f"ğŸ’¾ å…³é”®è¯ç´¢å¼•å·²ä¿å­˜: {keyword_path}")
        
        # æ„å»ºå‘é‡ç´¢å¼•
        if build_vector:
            indexed_chunks = self._create_vector_index()
            vector_path = output_path / "chunks_vector.json"
            with open(vector_path, 'w', encoding='utf-8') as f:
                json.dump(indexed_chunks, f, ensure_ascii=False)
            print(f"ğŸ’¾ å‘é‡ç´¢å¼•å·²ä¿å­˜: {vector_path}")
        
        return self.chunks
    
    def load_index(self, index_dir: str = "data"):
        """åŠ è½½å·²æœ‰ç´¢å¼•"""
        index_path = Path(index_dir)
        
        # åŠ è½½ chunks
        chunks_path = index_path / "chunks.json"
        if chunks_path.exists():
            with open(chunks_path, 'r', encoding='utf-8') as f:
                self.chunks = json.load(f)
            print(f"âœ… å·²åŠ è½½ {len(self.chunks)} ä¸ªæ–‡æœ¬å—")
        
        # åŠ è½½å…³é”®è¯ç´¢å¼•
        keyword_path = index_path / "keyword_index.json"
        if keyword_path.exists():
            with open(keyword_path, 'r', encoding='utf-8') as f:
                self.keyword_index = json.load(f)
            print(f"âœ… å·²åŠ è½½å…³é”®è¯ç´¢å¼•")


# ä¿æŒå‘åå…¼å®¹çš„åˆ«å
TextIndexer = UnifiedIndexer
VectorIndexer = UnifiedIndexer

if __name__ == "__main__":
    indexer = UnifiedIndexer(chunk_size=800, overlap=100)
    indexer.build_index("data/é›ªä¸­æ‚åˆ€è¡Œ.txt")