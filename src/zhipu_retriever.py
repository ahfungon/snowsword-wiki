#!/usr/bin/env python3
"""
æ™ºè°± AI Embedding è¯­ä¹‰æ£€ç´¢å™¨
ä½¿ç”¨æ™ºè°± GLM Embedding API è¿›è¡Œå‘é‡è¯­ä¹‰æ£€ç´¢
"""

import os
import json
import numpy as np
from pathlib import Path
from typing import List, Dict, Optional
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def get_zhipu_embedding(texts: List[str], api_key: str, model: str = "embedding-2") -> List[List[float]]:
    """
    è°ƒç”¨æ™ºè°± AI Embedding API
    æ–‡æ¡£: https://open.bigmodel.cn/dev/api#vector
    """
    import requests
    import time
    
    url = "https://open.bigmodel.cn/api/paas/v4/embeddings"
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    # æ™ºè°±ä¸€æ¬¡æœ€å¤š 8 æ¡
    batch_size = 8
    all_embeddings = []
    
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        
        payload = {
            "model": model,
            "input": batch
        }
        
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            
            if "data" in data:
                # æŒ‰ index æ’åº
                embeddings = sorted(data["data"], key=lambda x: x["index"])
                all_embeddings.extend([item["embedding"] for item in embeddings])
                logger.info(f"âœ… æ‰¹æ¬¡ {i//batch_size + 1} æˆåŠŸï¼Œ{len(batch)} æ¡")
            else:
                logger.error(f"âŒ API è¿”å›å¼‚å¸¸: {data}")
                raise ValueError(f"API è¿”å›å¼‚å¸¸: {data}")
            
            # ç®€å•é™é€Ÿ
            if i + batch_size < len(texts):
                time.sleep(0.5)
                
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
            raise
    
    return all_embeddings


class ZhipuEmbeddingRetriever:
    """
    æ™ºè°± Embedding è¯­ä¹‰æ£€ç´¢å™¨
    - ä½¿ç”¨æ™ºè°± GLM Embedding API ç¼–ç æ–‡æœ¬
    - å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢
    - è¯­ä¹‰ç†è§£ä¼˜äº TF-IDF
    """
    
    def __init__(self, api_key: str = None, model: str = "embedding-2"):
        self.api_key = api_key or os.getenv("ZHIPU_API_KEY")
        if not self.api_key:
            raise ValueError("éœ€è¦æä¾›æ™ºè°± AI API Key (ZHIPU_API_KEY)")
        
        self.model = model  # embedding-2 (1024ç»´) æˆ– embedding-3 (2048ç»´)
        
        self.embeddings = None
        self.paragraphs = []
        self.metadata = []
        self.dimension = None
    
    def build_index(self, paragraphs: List[Dict], output_dir: Path):
        """æ„å»ºå‘é‡ç´¢å¼•"""
        logger.info(f"ğŸ”¨ æ„å»ºæ™ºè°± Embedding ç´¢å¼•ï¼Œå…± {len(paragraphs)} ä¸ªæ®µè½ï¼Œæ¨¡å‹: {self.model}")
        
        # æå–æ–‡æœ¬
        texts = [p['content'] for p in paragraphs]
        
        # ä¿å­˜æ®µè½æ•°æ®ï¼ˆåªä¿å­˜æ–‡æœ¬å†…å®¹ï¼‰
        self.paragraphs = texts
        self.metadata = [
            {
                'idx': i,
                'chapter': p.get('chapter', ''),
                'chapter_idx': p.get('chapter_idx', 0),
                'paragraph_idx': p.get('paragraph_idx', 0),
            }
            for i, p in enumerate(paragraphs)
        ]
        
        # è·å– Embedding
        logger.info("ğŸ”„ è°ƒç”¨æ™ºè°± Embedding API...")
        embeddings_list = get_zhipu_embedding(texts, self.api_key, self.model)
        self.embeddings = np.array(embeddings_list)
        self.dimension = self.embeddings.shape[1]
        
        logger.info(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆ: {self.embeddings.shape}")
        
        # ä¿å­˜
        output_dir.mkdir(exist_ok=True, parents=True)
        
        # ä¿å­˜å‘é‡
        np.save(output_dir / 'embeddings.npy', self.embeddings)
        
        # ä¿å­˜å…ƒæ•°æ®
        with open(output_dir / 'metadata.json', 'w', encoding='utf-8') as f:
            json.dump({
                'paragraphs': texts,
                'metadata': self.metadata,
                'dimension': self.dimension,
                'model': self.model
            }, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ ç´¢å¼•å·²ä¿å­˜: {output_dir}")
    
    def load_index(self, index_dir: Path):
        """åŠ è½½ç´¢å¼•"""
        logger.info(f"ğŸ“‚ åŠ è½½æ™ºè°± Embedding ç´¢å¼•: {index_dir}")
        
        # åŠ è½½å‘é‡
        self.embeddings = np.load(index_dir / 'embeddings.npy')
        self.dimension = self.embeddings.shape[1]
        
        # åŠ è½½å…ƒæ•°æ®
        with open(index_dir / 'metadata.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
            self.paragraphs = data['paragraphs']
            self.metadata = data['metadata']
            saved_model = data.get('model', 'unknown')
        
        logger.info(f"âœ… ç´¢å¼•åŠ è½½å®Œæˆ: {self.embeddings.shape}, æ¨¡å‹: {saved_model}")
    
    def _cosine_similarity(self, query_vec: np.ndarray) -> np.ndarray:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        # å½’ä¸€åŒ–
        query_vec = query_vec / (np.linalg.norm(query_vec) + 1e-8)
        embeddings_norm = self.embeddings / (np.linalg.norm(self.embeddings, axis=1, keepdims=True) + 1e-8)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = np.dot(embeddings_norm, query_vec)
        return similarities
    
    def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """è¯­ä¹‰æ£€ç´¢"""
        if self.embeddings is None:
            logger.error("âŒ ç´¢å¼•æœªåŠ è½½")
            return []
        
        # ç¼–ç æŸ¥è¯¢
        logger.info(f"ğŸ” ç¼–ç æŸ¥è¯¢: {query[:50]}...")
        query_embedding = get_zhipu_embedding([query], self.api_key, self.model)[0]
        query_vec = np.array(query_embedding)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = self._cosine_similarity(query_vec)
        
        # å– top-k
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        # ç»„è£…ç»“æœ
        results = []
        for idx in top_indices:
            results.append({
                'idx': int(idx),
                'similarity': float(similarities[idx]),
                'content': self.paragraphs[idx],
                **self.metadata[idx]
            })
        
        return results
    
    def hybrid_search(self, query: str, top_k: int = 5, alpha: float = 0.8) -> List[Dict]:
        """æ··åˆæ£€ç´¢ï¼šè¯­ä¹‰ + å…³é”®è¯"""
        import jieba
        
        # è¯­ä¹‰åˆ†æ•°
        query_embedding = get_zhipu_embedding([query], self.api_key, self.model)[0]
        query_vec = np.array(query_embedding)
        semantic_scores = self._cosine_similarity(query_vec)
        
        # å…³é”®è¯åˆ†æ•°
        keywords = set(jieba.cut(query))
        keyword_scores = np.zeros(len(self.paragraphs))
        
        for i, content in enumerate(self.paragraphs):
            score = sum(1 for kw in keywords if kw in content)
            keyword_scores[i] = score / max(len(keywords), 1)
        
        # å½’ä¸€åŒ–å…³é”®è¯åˆ†æ•°
        if keyword_scores.max() > 0:
            keyword_scores = keyword_scores / keyword_scores.max()
        
        # èåˆï¼ˆè¯­ä¹‰æƒé‡ alphaï¼Œå…³é”®è¯æƒé‡ 1-alphaï¼‰
        combined_scores = alpha * semantic_scores + (1 - alpha) * keyword_scores
        
        # å– top-k
        top_indices = np.argsort(combined_scores)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            results.append({
                'idx': int(idx),
                'similarity': float(combined_scores[idx]),
                'semantic_score': float(semantic_scores[idx]),
                'keyword_score': float(keyword_scores[idx]),
                'content': self.paragraphs[idx],
                **self.metadata[idx]
            })
        
        return results


if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯•æ™ºè°± Embedding æ£€ç´¢å™¨")
    print("="*60)
    
    # æµ‹è¯•æ•°æ®
    test_paragraphs = [
        {'content': 'å¾å‡¤å¹´ä¸ºæ¯æŠ¥ä»‡ï¼Œåœ¨å¤ªå®‰åŸå¤–æ–©æ€éŸ©è²‚å¯º', 'chapter': 'ç¬¬100ç« '},
        {'content': 'å§œæ³¥å’Œå¾å‡¤å¹´é’æ¢…ç«¹é©¬ï¼Œæœ€ç»ˆæˆä¸ºåŒ—å‡‰ç‹å¦ƒ', 'chapter': 'ç¬¬200ç« '},
        {'content': 'ææ·³ç½¡ä¼ æˆå¾å‡¤å¹´ä¸¤è¢–é’è›‡ï¼Œå‰‘é“å¤§æˆ', 'chapter': 'ç¬¬50ç« '},
        {'content': 'ç‹ä»™èŠè‡ªç§°å¤©ä¸‹ç¬¬äºŒï¼Œé•‡å®ˆæ­¦å¸åŸä¸€ç”²å­', 'chapter': 'ç¬¬150ç« '},
        {'content': 'å¾éªäººå± ä¹‹åå¨éœ‡å¤©ä¸‹ï¼Œå®ˆæŠ¤åŒ—å‡‰ä¸‰åå¹´', 'chapter': 'ç¬¬10ç« '},
    ]
    
    # åˆ›å»ºæ£€ç´¢å™¨
    api_key = os.getenv("ZHIPU_API_KEY")
    if not api_key:
        print("âš ï¸ è¯·è®¾ç½® ZHIPU_API_KEY ç¯å¢ƒå˜é‡")
        exit(1)
    
    retriever = ZhipuEmbeddingRetriever(api_key=api_key)
    
    # æ„å»ºç´¢å¼•
    retriever.build_index(test_paragraphs, Path("test_index_zhipu"))
    
    # æµ‹è¯•æŸ¥è¯¢
    queries = [
        "å¾å‡¤å¹´ä¸ºä»€ä¹ˆè¦æ€éŸ©è²‚å¯ºï¼Ÿ",
        "å§œæ³¥å’Œå¾å‡¤å¹´çš„å…³ç³»ï¼Ÿ",
        "è°æ•™å¾å‡¤å¹´å‰‘æ³•ï¼Ÿ",
    ]
    
    for query in queries:
        print(f"\nğŸ” æŸ¥è¯¢: {query}")
        results = retriever.search(query, top_k=2)
        for r in results:
            print(f"   [{r['idx']}] {r['content'][:40]}... (ç›¸ä¼¼åº¦: {r['similarity']:.3f})")
