#!/usr/bin/env python3
"""
è½»é‡è¯­ä¹‰æ£€ç´¢å™¨ - ä½¿ç”¨ TF-IDF + è¯é¢‘
æ— éœ€ä¸‹è½½å¤§æ¨¡å‹ï¼Œå¿«é€Ÿå¯ç”¨
"""

import json
import numpy as np
from pathlib import Path
from typing import List, Dict
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import jieba
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class LightweightRetriever:
    """
    è½»é‡çº§æ£€ç´¢å™¨
    - TF-IDF å‘é‡åŒ–
    - ä½™å¼¦ç›¸ä¼¼åº¦æ£€ç´¢
    - æ— éœ€æ·±åº¦å­¦ä¹ æ¨¡å‹
    """
    
    def __init__(self):
        self.vectorizer = TfidfVectorizer(
            tokenizer=self._tokenize,
            max_features=10000,  # é™åˆ¶ç‰¹å¾æ•°
            ngram_range=(1, 2),  # 1-2 gram
            min_df=2,  # è‡³å°‘å‡ºç°2æ¬¡
            max_df=0.8  # æœ€å¤š80%çš„æ–‡æ¡£
        )
        self.tfidf_matrix = None
        self.paragraphs = []
        self.metadata = []
    
    def _tokenize(self, text: str) -> List[str]:
        """ä¸­æ–‡åˆ†è¯"""
        return list(jieba.cut(text))
    
    def build_index(self, paragraphs: List[Dict], output_dir: Path):
        """æ„å»º TF-IDF ç´¢å¼•"""
        logger.info(f"ğŸ”¨ æ„å»ºè½»é‡ç´¢å¼•ï¼Œå…± {len(paragraphs)} ä¸ªæ®µè½")
        
        # ä¿å­˜æ®µè½æ•°æ®
        self.paragraphs = paragraphs
        self.metadata = [
            {
                'idx': i,
                'chapter': p.get('chapter', ''),
                'chapter_idx': p.get('chapter_idx', 0),
                'paragraph_idx': p.get('paragraph_idx', 0),
            }
            for i, p in enumerate(paragraphs)
        ]
        
        # æå–æ–‡æœ¬
        texts = [p['content'] for p in paragraphs]
        
        # æ„å»º TF-IDF çŸ©é˜µ
        logger.info("ğŸ”„ è®¡ç®— TF-IDF...")
        self.tfidf_matrix = self.vectorizer.fit_transform(texts)
        
        logger.info(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆ: {self.tfidf_matrix.shape}")
        
        # ä¿å­˜
        output_dir.mkdir(exist_ok=True, parents=True)
        
        # ä¿å­˜å‘é‡åŒ–å™¨
        import pickle
        with open(output_dir / 'vectorizer.pkl', 'wb') as f:
            pickle.dump(self.vectorizer, f)
        
        # ä¿å­˜ TF-IDF çŸ©é˜µ
        from scipy.sparse import save_npz
        save_npz(output_dir / 'tfidf_matrix.npz', self.tfidf_matrix)
        
        # ä¿å­˜å…ƒæ•°æ®
        with open(output_dir / 'metadata.json', 'w', encoding='utf-8') as f:
            json.dump({
                'paragraphs': texts,
                'metadata': self.metadata
            }, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ ç´¢å¼•å·²ä¿å­˜: {output_dir}")
    
    def load_index(self, index_dir: Path):
        """åŠ è½½ç´¢å¼•"""
        logger.info(f"ğŸ“‚ åŠ è½½ç´¢å¼•: {index_dir}")
        
        import pickle
        from scipy.sparse import load_npz
        
        # åŠ è½½å‘é‡åŒ–å™¨
        with open(index_dir / 'vectorizer.pkl', 'rb') as f:
            self.vectorizer = pickle.load(f)
        
        # åŠ è½½ TF-IDF çŸ©é˜µ
        self.tfidf_matrix = load_npz(index_dir / 'tfidf_matrix.npz')
        
        # åŠ è½½å…ƒæ•°æ®
        with open(index_dir / 'metadata.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
            self.paragraphs = data['paragraphs']
            self.metadata = data['metadata']
        
        logger.info(f"âœ… ç´¢å¼•åŠ è½½å®Œæˆ: {self.tfidf_matrix.shape}")
    
    def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """æ£€ç´¢"""
        if self.tfidf_matrix is None:
            logger.error("âŒ ç´¢å¼•æœªåŠ è½½")
            return []
        
        # ç¼–ç æŸ¥è¯¢
        query_vec = self.vectorizer.transform([query])
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = cosine_similarity(query_vec, self.tfidf_matrix).flatten()
        
        # å– top-k
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        # ç»„è£…ç»“æœ
        results = []
        for idx in top_indices:
            results.append({
                'idx': int(idx),
                'similarity': float(similarities[idx]),
                'content': self.paragraphs[idx],
                **self.metadata[idx]
            })
        
        return results
    
    def hybrid_search(self, query: str, top_k: int = 5) -> List[Dict]:
        """æ··åˆæ£€ç´¢ï¼šTF-IDF + å…³é”®è¯åŠ æƒ"""
        # TF-IDF åˆ†æ•°
        query_vec = self.vectorizer.transform([query])
        tfidf_scores = cosine_similarity(query_vec, self.tfidf_matrix).flatten()
        
        # å…³é”®è¯åˆ†æ•°
        keywords = set(jieba.cut(query))
        keyword_scores = np.zeros(len(self.paragraphs))
        
        for i, content in enumerate(self.paragraphs):
            score = sum(1 for kw in keywords if kw in content)
            keyword_scores[i] = score / max(len(keywords), 1)
        
        # èåˆï¼ˆTF-IDF æƒé‡ 0.7ï¼Œå…³é”®è¯ 0.3ï¼‰
        combined_scores = 0.7 * tfidf_scores + 0.3 * keyword_scores
        
        # å– top-k
        top_indices = np.argsort(combined_scores)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            results.append({
                'idx': int(idx),
                'similarity': float(combined_scores[idx]),
                'tfidf_score': float(tfidf_scores[idx]),
                'keyword_score': float(keyword_scores[idx]),
                'content': self.paragraphs[idx],
                **self.metadata[idx]
            })
        
        return results


if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯•è½»é‡çº§æ£€ç´¢å™¨")
    print("="*60)
    
    # æµ‹è¯•æ•°æ®
    test_paragraphs = [
        {'content': 'å¾å‡¤å¹´ä¸ºæ¯æŠ¥ä»‡ï¼Œåœ¨å¤ªå®‰åŸå¤–æ–©æ€éŸ©è²‚å¯º', 'chapter': 'ç¬¬100ç« '},
        {'content': 'å§œæ³¥å’Œå¾å‡¤å¹´é’æ¢…ç«¹é©¬ï¼Œæœ€ç»ˆæˆä¸ºåŒ—å‡‰ç‹å¦ƒ', 'chapter': 'ç¬¬200ç« '},
        {'content': 'ææ·³ç½¡ä¼ æˆå¾å‡¤å¹´ä¸¤è¢–é’è›‡ï¼Œå‰‘é“å¤§æˆ', 'chapter': 'ç¬¬50ç« '},
        {'content': 'ç‹ä»™èŠè‡ªç§°å¤©ä¸‹ç¬¬äºŒï¼Œé•‡å®ˆæ­¦å¸åŸä¸€ç”²å­', 'chapter': 'ç¬¬150ç« '},
        {'content': 'å¾éªäººå± ä¹‹åå¨éœ‡å¤©ä¸‹ï¼Œå®ˆæŠ¤åŒ—å‡‰ä¸‰åå¹´', 'chapter': 'ç¬¬10ç« '},
    ]
    
    # åˆ›å»ºæ£€ç´¢å™¨
    retriever = LightweightRetriever()
    
    # æ„å»ºç´¢å¼•
    retriever.build_index(test_paragraphs, Path("test_index_light"))
    
    # æµ‹è¯•æŸ¥è¯¢
    queries = [
        "å¾å‡¤å¹´ä¸ºä»€ä¹ˆè¦æ€éŸ©è²‚å¯ºï¼Ÿ",
        "å§œæ³¥å’Œå¾å‡¤å¹´çš„å…³ç³»ï¼Ÿ",
        "è°æ•™å¾å‡¤å¹´å‰‘æ³•ï¼Ÿ",
    ]
    
    for query in queries:
        print(f"\nğŸ” æŸ¥è¯¢: {query}")
        results = retriever.search(query, top_k=2)
        for r in results:
            print(f"   [{r['idx']}] {r['content'][:40]}... (ç›¸ä¼¼åº¦: {r['similarity']:.3f})")
