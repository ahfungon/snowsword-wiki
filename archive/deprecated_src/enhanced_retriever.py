#!/usr/bin/env python3
"""
å¢å¼ºæ£€ç´¢å™¨ - é›†æˆå‘é‡æ£€ç´¢ã€çŸ¥è¯†å›¾è°±ã€ç« èŠ‚æ‘˜è¦
"""

import json
import gzip
import os
import subprocess
from pathlib import Path
from typing import List, Dict, Tuple
import numpy as np
import jieba


class EnhancedRetriever:
    """å¢å¼ºç‰ˆæ£€ç´¢å™¨"""
    
    def __init__(self, data_dir: str = "data"):
        self.data_dir = Path(data_dir)
        self.chunks = []
        self.chunk_map = {}
        self.vector_index = []
        self.knowledge_graph = None
        self.chapter_summaries = []
        
        # åŠ è½½æ•°æ®
        self._load_data()
    
    def _load_data(self):
        """åŠ è½½æ‰€æœ‰æ•°æ®"""
        print("ğŸ”„ æ­£åœ¨åŠ è½½å¢å¼ºç´¢å¼•...")
        
        # 1. åŠ è½½æ–‡æœ¬å—ï¼ˆæ”¯æŒå‹ç¼©æ–‡ä»¶è§£å‹ï¼‰
        self._load_chunks()
        
        # 2. åŠ è½½çŸ¥è¯†å›¾è°±
        self._load_knowledge_graph()
        
        # 3. åŠ è½½ç« èŠ‚æ‘˜è¦
        self._load_chapter_summaries()
        
        print(f"âœ… ç´¢å¼•åŠ è½½å®Œæˆ")
    
    def _load_chunks(self):
        """åŠ è½½æ–‡æœ¬å—ï¼Œæ”¯æŒå‹ç¼©æ–‡ä»¶"""
        chunks_path = self.data_dir / "chunks.json"
        
        # æ£€æŸ¥å‹ç¼©æ–‡ä»¶
        compressed_files = sorted(self.data_dir.glob("chunks_small_*.gz"))
        
        if not chunks_path.exists() and compressed_files:
            print(f"ğŸ“¦ å‘ç° {len(compressed_files)} ä¸ªå‹ç¼©æ–‡ä»¶ï¼Œå¼€å§‹è§£å‹...")
            self._decompress_and_merge(compressed_files, chunks_path)
        
        if chunks_path.exists():
            print(f"ğŸ“– åŠ è½½æ–‡æœ¬å—...")
            with open(chunks_path, 'r', encoding='utf-8') as f:
                self.chunks = json.load(f)
            self.chunk_map = {chunk['id']: chunk for chunk in self.chunks}
            print(f"   âœ“ åŠ è½½äº† {len(self.chunks)} ä¸ªæ–‡æœ¬å—")
    
    def _decompress_and_merge(self, compressed_files: List[Path], output_path: Path):
        """è§£å‹å¹¶åˆå¹¶æ–‡ä»¶"""
        temp_dir = self.data_dir / "temp_chunks"
        temp_dir.mkdir(exist_ok=True)
        
        total = len(compressed_files)
        for i, gz_file in enumerate(compressed_files, 1):
            print(f"   [{i}/{total}] è§£å‹ {gz_file.name}...")
            output_file = temp_dir / gz_file.stem
            with gzip.open(gz_file, 'rb') as f_in:
                with open(output_file, 'wb') as f_out:
                    f_out.write(f_in.read())
        
        # åˆå¹¶
        print(f"ğŸ“‘ åˆå¹¶ {total} ä¸ªæ–‡ä»¶...")
        split_files = sorted(temp_dir.glob("chunks_small_*"))
        files_str = ' '.join([str(f) for f in split_files])
        cmd = f"cat {files_str} > {output_path}"
        subprocess.run(cmd, shell=True, check=True)
        
        # æ¸…ç†
        for f in split_files:
            f.unlink()
        temp_dir.rmdir()
        
        print(f"   âœ“ è§£å‹åˆå¹¶å®Œæˆ")
    
    def _load_knowledge_graph(self):
        """åŠ è½½çŸ¥è¯†å›¾è°±"""
        kg_path = self.data_dir / "knowledge_graph.json"
        if kg_path.exists():
            with open(kg_path, 'r', encoding='utf-8') as f:
                self.knowledge_graph = json.load(f)
            print(f"   âœ“ çŸ¥è¯†å›¾è°±: {len(self.knowledge_graph.get('characters', {}))} ä¸ªäººç‰©")
    
    def _load_chapter_summaries(self):
        """åŠ è½½ç« èŠ‚æ‘˜è¦"""
        summary_path = self.data_dir / "chapter_summaries.json"
        if summary_path.exists():
            with open(summary_path, 'r', encoding='utf-8') as f:
                self.chapter_summaries = json.load(f)
            print(f"   âœ“ ç« èŠ‚æ‘˜è¦: {len(self.chapter_summaries)} ç« ")
    
    def _get_embedding(self, text: str) -> List[float]:
        """è·å–æ–‡æœ¬å‘é‡ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        words = set(text[:200])
        vector = [ord(char) % 100 / 100.0 for char in words]
        while len(vector) < 128:
            vector.append(0.0)
        return vector[:128]
    
    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        a = np.array(vec1)
        b = np.array(vec2)
        norm_a = np.linalg.norm(a)
        norm_b = np.linalg.norm(b)
        if norm_a == 0 or norm_b == 0:
            return 0.0
        return np.dot(a, b) / (norm_a * norm_b)
    
    def _extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯"""
        words = jieba.lcut(text)
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        stopwords = {'çš„', 'æ˜¯', 'åœ¨', 'å’Œ', 'äº†', 'æœ‰', 'æˆ‘', 'éƒ½', 'ä¸ª', 'ä¸', 'ä¹Ÿ', 'å¯¹', 'ä¸º', 'èƒ½', 'å¾ˆ', 'å¯ä»¥', 'å°±', 'ä¸', 'ä¼š', 'è¦', 'æ²¡æœ‰', 'åˆ°', 'æ›´', 'è®©', 'ä½†', 'ç»™', 'ä¸Š', 'è¿™', 'ä»–', 'å¥¹', 'å®ƒ', 'ä»¬', 'ä½ ', 'æ‚¨', 'æˆ‘ä»¬', 'ä½ ä»¬', 'ä»–ä»¬', 'å¥¹ä»¬', 'å®ƒä»¬'}
        keywords = [w.strip() for w in words if len(w.strip()) > 1 and w.strip() not in stopwords]
        return keywords
    
    def retrieve(self, query: str, top_k: int = 5) -> List[Tuple[Dict, float]]:
        """
        å¤šç­–ç•¥æ£€ç´¢ï¼š
        1. å‘é‡ç›¸ä¼¼åº¦
        2. å…³é”®è¯åŒ¹é…
        3. çŸ¥è¯†å›¾è°±å¢å¼º
        """
        results = []
        
        # 1. å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢
        query_vector = self._get_embedding(query)
        vector_scores = []
        
        for chunk in self.chunks:
            chunk_vector = self._get_embedding(chunk['content'])
            similarity = self._cosine_similarity(query_vector, chunk_vector)
            vector_scores.append((chunk, similarity))
        
        vector_scores.sort(key=lambda x: x[1], reverse=True)
        
        # 2. å…³é”®è¯åŒ¹é…
        query_keywords = set(self._extract_keywords(query))
        keyword_scores = []
        
        for chunk in self.chunks:
            chunk_text = chunk['content']
            chunk_keywords = set(self._extract_keywords(chunk_text))
            
            # è®¡ç®—åŒ¹é…åº¦
            if query_keywords:
                match_ratio = len(query_keywords & chunk_keywords) / len(query_keywords)
            else:
                match_ratio = 0
            
            keyword_scores.append((chunk, match_ratio))
        
        keyword_scores.sort(key=lambda x: x[1], reverse=True)
        
        # 3. çŸ¥è¯†å›¾è°±å¢å¼º
        # æå–æŸ¥è¯¢ä¸­çš„äººç‰©
        mentioned_chars = []
        if self.knowledge_graph:
            for char in self.knowledge_graph.get('characters', {}):
                if char in query:
                    mentioned_chars.append(char)
        
        # 4. èåˆæ’åº
        chunk_scores = {}
        
        # å‘é‡åˆ†æ•°æƒé‡
        for i, (chunk, score) in enumerate(vector_scores[:top_k * 2]):
            chunk_id = chunk['id']
            chunk_scores[chunk_id] = chunk_scores.get(chunk_id, 0) + score * 0.4
        
        # å…³é”®è¯åˆ†æ•°æƒé‡
        for i, (chunk, score) in enumerate(keyword_scores[:top_k * 2]):
            chunk_id = chunk['id']
            chunk_scores[chunk_id] = chunk_scores.get(chunk_id, 0) + score * 0.4
        
        # äººç‰©åŒ¹é…åŠ åˆ†
        for chunk in self.chunks:
            chunk_id = chunk['id']
            for char in mentioned_chars:
                if char in chunk['content']:
                    chunk_scores[chunk_id] = chunk_scores.get(chunk_id, 0) + 0.2
        
        # æ’åºå¹¶è¿”å› top_k
        sorted_chunks = sorted(chunk_scores.items(), key=lambda x: x[1], reverse=True)
        
        for chunk_id, score in sorted_chunks[:top_k]:
            if chunk_id in self.chunk_map:
                results.append((self.chunk_map[chunk_id], score))
        
        return results
    
    def get_context(self, query: str, top_k: int = 5) -> str:
        """è·å–å¢å¼ºä¸Šä¸‹æ–‡"""
        # è·å–æ£€ç´¢ç»“æœ
        results = self.retrieve(query, top_k=top_k)
        
        # æå–æŸ¥è¯¢ä¸­çš„äººç‰©
        mentioned_chars = []
        if self.knowledge_graph:
            for char in self.knowledge_graph.get('characters', {}):
                if char in query:
                    mentioned_chars.append(char)
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        
        # 1. çŸ¥è¯†å›¾è°±ä¿¡æ¯
        if mentioned_chars and self.knowledge_graph:
            context_parts.append("ã€äººç‰©èƒŒæ™¯ã€‘")
            for char in mentioned_chars:
                info = self.knowledge_graph['characters'].get(char, {})
                if info:
                    identity = info.get('identity', '')
                    faction = info.get('faction', '')
                    context_parts.append(f"{char}ï¼š{identity}ï¼Œæ‰€å±ï¼š{faction}")
            context_parts.append("")
        
        # 2. åŸæ–‡ç‰‡æ®µ
        context_parts.append("ã€ç›¸å…³åŸæ–‡ã€‘")
        for i, (chunk, score) in enumerate(results, 1):
            context_parts.append(f"ç‰‡æ®µ{i}ï¼ˆç›¸å…³åº¦ï¼š{score:.2f}ï¼‰ï¼š")
            context_parts.append(chunk['content'])
            context_parts.append("")
        
        return "\n".join(context_parts)
    
    def get_chapter_summary(self, chapter_title: str) -> Dict:
        """è·å–ç« èŠ‚æ‘˜è¦"""
        for summary in self.chapter_summaries:
            if chapter_title in summary['title']:
                return summary
        return {}


if __name__ == "__main__":
    # æµ‹è¯•
    print("ğŸ§ª æµ‹è¯•å¢å¼ºæ£€ç´¢å™¨...")
    retriever = EnhancedRetriever()
    
    query = "å¾å‡¤å¹´å’Œæ‹“è·‹è©è¨çš„å…³ç³»"
    print(f"\nğŸ” æŸ¥è¯¢: {query}")
    
    results = retriever.retrieve(query, top_k=3)
    print(f"\næ£€ç´¢ç»“æœ:")
    for chunk, score in results:
        print(f"  [{chunk['chapter']}] ç›¸å…³åº¦: {score:.2f}")
        print(f"    {chunk['content'][:80]}...")
    
    print(f"\nä¸Šä¸‹æ–‡é¢„è§ˆ:")
    context = retriever.get_context(query, top_k=2)
    print(context[:500])
