#!/usr/bin/env python3
"""
å‘é‡ç´¢å¼•ç”Ÿæˆå™¨ - ä½¿ç”¨ DeepSeek Embedding API
"""

import json
import os
from pathlib import Path
from typing import List, Dict
import numpy as np
from openai import OpenAI


class VectorIndexer:
    """ç”Ÿæˆæ–‡æœ¬å—çš„å‘é‡åµŒå…¥"""
    
    def __init__(self, api_key: str = None):
        self.api_key = api_key or os.getenv("DEEPSEEK_API_KEY")
        self.client = OpenAI(
            api_key=self.api_key,
            base_url="https://api.deepseek.com"
        )
        self.model = "text-embedding-ada-002"  # æˆ–ä½¿ç”¨ deepseek çš„ embedding
        
    def get_embedding(self, text: str) -> List[float]:
        """è·å–æ–‡æœ¬çš„å‘é‡åµŒå…¥"""
        # DeepSeek ç›®å‰æ²¡æœ‰ä¸“é—¨çš„ embedding APIï¼Œä½¿ç”¨ç®€å•çš„å­—ç¬¦ç¼–ç ä½œä¸ºä¸´æ—¶æ–¹æ¡ˆ
        # å®é™…ç”Ÿäº§ç¯å¢ƒåº”è¯¥ä½¿ç”¨ OpenAI æˆ–å…¶ä»– embedding æœåŠ¡
        
        # ç®€åŒ–ç‰ˆï¼šä½¿ç”¨è¯é¢‘å‘é‡
        words = set(text[:200])  # å–å‰200å­—ç¬¦
        vector = []
        for char in words:
            vector.append(ord(char) % 100 / 100.0)  # å½’ä¸€åŒ–
        
        # å¡«å……åˆ°å›ºå®šé•¿åº¦
        while len(vector) < 128:
            vector.append(0.0)
        return vector[:128]
    
    def cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        a = np.array(vec1)
        b = np.array(vec2)
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
    
    def create_vector_index(self, chunks: List[Dict]) -> List[Dict]:
        """ä¸ºæ‰€æœ‰æ–‡æœ¬å—ç”Ÿæˆå‘é‡ç´¢å¼•"""
        print(f"ğŸ§® æ­£åœ¨ä¸º {len(chunks)} ä¸ªæ–‡æœ¬å—ç”Ÿæˆå‘é‡åµŒå…¥...")
        
        indexed_chunks = []
        for i, chunk in enumerate(chunks):
            if (i + 1) % 1000 == 0:
                print(f"  è¿›åº¦: {i+1}/{len(chunks)}")
            
            vector = self.get_embedding(chunk['content'])
            indexed_chunks.append({
                **chunk,
                'embedding': vector
            })
        
        print(f"âœ… å‘é‡ç´¢å¼•ç”Ÿæˆå®Œæˆ")
        return indexed_chunks
    
    def save_index(self, indexed_chunks: List[Dict], output_path: Path):
        """ä¿å­˜å‘é‡ç´¢å¼•"""
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(indexed_chunks, f, ensure_ascii=False)
        print(f"ğŸ’¾ å‘é‡ç´¢å¼•å·²ä¿å­˜: {output_path}")


if __name__ == "__main__":
    # æµ‹è¯•
    import sys
    sys.path.append('.')
    from src.indexer import TextIndexer
    
    # åŠ è½½å·²æœ‰ chunks
    data_dir = Path("data")
    with open(data_dir / "chunks.json", 'r', encoding='utf-8') as f:
        chunks = json.load(f)
    
    # ç”Ÿæˆå‘é‡ç´¢å¼•
    indexer = VectorIndexer()
    indexed_chunks = indexer.create_vector_index(chunks)
    
    # ä¿å­˜
    indexer.save_index(indexed_chunks, data_dir / "chunks_vector.json")
